# CS82 Introduction to Machine Learning
## Intro to Machine Learning

Machine Learning is a method by which we can solve problems without relying on specific instructions on how to solve those problems. Machine Learning requires data that can be used to "train" algorithms in solving the problem we are interested in, and then allow us to make inference about other data. 

The appeal of Machine Learning is that it is a very robust method in solving problems as compared to trying to solve a problem using rule-based approaches. In some cases the same problems that we can solve using Machine Learning techniques can be solved using multiple `if` statements, however determining what those if statements should be, and making our program resistant to changes in the enviroment is a tedious task. That would be a rule based approach, we create a rule for each possible case we encounter.

Because it is impractical to solve a problem using multiple rules, which we first have to determine, as well as adapt our rules in an ever changing world we use Machine Learning. Imagine that because of inflation the house pricing program we build using if-statements starts giving very erronous results. A Machine Learning model can update itself on the contrary without any intervation from us, apart from supplying it with the data. 

These solutions are generated from **Data** that correspond to a problem. We can't teach machines without some type of data. 

There are different fields within Machine Learning and Artificial Intelligence. The ones we will study in this class are concerned with **Supervised Learning, Unsupervised Learning and Reinforcement Learning.**
![](https://i.imgur.com/BRm3dXN.png)

**Supervised Learning** is a method of tuning our algorithms by providing them examples. If we try to predict the price a house could sell, we can use data from previous sales. Such data can include the number of rooms, the number of bathrooms, square feet, lot size of previous sold houses and the price they sold for. Such a problem is called a **Regression** problem. For **Regression** problems we try to predict a value, example $1023.23 or 84.4 degrees Fahrenheit.
**Classification** problems on the other hand are problems for which we try to classify something into multiple classes. An example of classification problem is to predict if tomorrow will be sunny, rainy or cloudy.

**Unsupervised Learning** is a method of learning from the data. We don't provide explicit information on the structure of data. As an example consider **Clustering**, we try to cluster many data points together based on how close they are to each other. The result of unsupervised learning is usually more structured data than what we started with. Dimensionality reduction provides structure to data by removing dimensions of the data that doesn't add information to our understanding of the data.

**Reinforcement Learning** is a method of learning from data by which we teach agents to act in environments. Such problem solving techniques are usually used when there are agents in unobservable or unpredictable environments. For example consider when you play with AI on a video game. A properly build AI would use past data on how you play to calibrate its game style to your play style.

In this class we will briefly cover all of those topics and provide a foundation for solving problems in all of those domains.

## Week 4 - Regression

___

## Contents:
 * Equation of a Line
 * Linear Model  
 * Cost function
   - Model Evaluation (Least Square Method)
* SKlearn
* Nonlinear Regression


### Regression

​	Regression helps solve problems that are concerned with predicting a **real** value. A real value is any number, integer, float, or even boolean values are all real values. Real values denoted as $\R$, are continious and are infenetely many. 

​	Be careful however, not all number problems are regression problems. Assume we are trying to predict hand written digits from a picture. The number of possible values is limited, we only have 10 possible digits, 0 through 9. The solution is a value but this is not a regression problem, it is a classification problem.  Regression can still be used but then we will have to change on how we interpert the results, for example we can have the regression return a real value that we could interpert as a probability belonging to a specific number. However this is not practical or useful.

​	On the contrary, a regression problem has outputs that are infenetely many. For example what price to sell a house at? We could list that house for `$300.5` or for `$300.52 ` or for `$300.521` as you can see for this problem I can generate infenetely many answers. I could go on the other extreme and convert this problem in a classification problem by creating a seperate class for each float number, this would be possible but again impractical. 

​	Deciding when to solve a problem by Regression, has to do with **What are you trying to answer?**

### Linear Regression

Regression problems where there is a linear relationship between the attributes of your data. Consider the example of hours we study and the score percentage we get in that test.
![](https://lazyprogrammer.me/wp-content/uploads/2014/11/linear-regression-python-scikit-learn-1.png)

There is a linear relationship between the hours we study and the score pecentage. This can easily be visualized by a single line that can be fit to the data. We can see that a single straight line fits the data fairly well.

### NonLinear Regression

Problems for which the data can't be described by a linear relationship. An example of data that follow a repetitive pattern is shown below. 

![](https://blogs.mathworks.com/images/loren/385/fit12a_06.png)
Such an example of a dataset could be the temprature year-round. There is no single line that can be fit in the data that will help us make accurate predictions.



Deciding if the data attributes have a linear or non-linear relationship with our depedent variable can be as easy as plotting our dependent variable (what we are trying to predict) and our indepedent variable (an attribute from the data).

### Equation of  Line

The general equation of a 2d line is given by $f(x)=m\times x+b$

The equation of a 2d line can work well when only have a single attribute that we are using to make our prediction. As the example above we were attempting to predict our score given the hours we studied. 

To solve this Linear Regression problem, we need to find the unknowns in our line equation. In this case is $m$ and $b$

After we have the slope of the line $m$ and the y-intercept $b$ , we can input in our very simplistic model, the number of hours we studied denoted by $x$ and we should get as an answer our score $f(x)$

### Machine Learning for Regression

The Machine learning aspect of the problem described above is finding the slope $m$ and the intercept $b$, to do this we need to present to our Linear Regression model, **data** of previous scores as well as the hours spend studying to achieve those scores. 

#### Example

Assume we have fitted our model and we have found that $b=10$ and $m=4$, we can do some simple calculations to figure out what our score would be. 

$f(x)=2\times x+10$ , if we study 20 hours for this exam, we should expect a score of 90, while if we spend 0 hours studying for this exam we should expect a score of 10. 

#### Note

The *problem* (if we even consider it one) with this approach is that studying 20 hours or not at all doesn't guarantee us a score. It is a prediction of how well we are expected to do. We might spend 40 hours studying and perform as well as if we spend 20 hours studying. A score above 100 would not be possible in this case. Such models come with restrictions, and understanding the problem we are trying to solve is more important than implementing the model. 

We can generate a **random** linear regression for demonstration purposes as follow. We obtain the **ground truth** (the real) coefficient of our linear regression and we plot our results below. This is pretty typical of a real world problem that we can solve with linear regression. There are outliers or other factors that influence deviations from our linear prediction. Some times it is the nature of the world we live in which is stochastic.

```python
from sklearn.datasets.samples_generator import make_regression
import matplotlib.pyplot as plt
import numpy as np
# Make regression returns random points that follow along a regression line. The arguments we pass are important
# n_samples signifies how many data points to generate
# n_features is the number of attributes we want to generate, in this case 1 attribute
# noise is how much random noise we want our data to have
# random_state helps prevent the function return random results every time and allow us to be consistent with one another
# coef returns the coefficient of the linear regression generated
reg=make_regression(n_samples=100, n_features=1, noise=20,coef=True,random_state=0)
plt.scatter(x=reg[0],y=reg[1])
m=reg[2]
x=np.linspace(-3,3,num=10)
fx=m*x
# the c parameter is the color, and in this case we specify red
plt.plot(x,fx, c='r')
```

![image-20190719181816881](../image/image-20190719181816881.png)

All of the points follow a pattern of a line but not all of them fall exactly on the line. However if we were to predict the score based on the hours a student has studied we would be on average closer than any other estimate we would try to make.

Being spot on accurate about the exact value of this regression is not what we are concerned in such cases. It would be impossible to predict for example the score completely based on the hours studied, there are so many factors that go into the score that we are simply unable to capture them. Machine Learning is ideal for such kind of problems for which there is a relationship between two variables and we can create a model to capture that relationship with **varying degrees of accuracy**.

### Multivariate Linear Regression

​	Multivariete signifies models that use more than one attribute to generate a prediction. Rarely if ever we will have a single attribute we are using in our model. Most of the times we will have multiple variables, hence the name **Multivariable regression**. 

​	Consider the example of trying to predict the score on a test. Maybe we want to consider the student's current GPA in our model to improve it, or whether the student ate breakfast that morning, or how the student did in a previous test. As long as we have accurate data for what we are collecting, we can consider as many attributes as we wis.

​	 **Important Note** adding many attributes to our data that are random or unrelated can make our predictions worse by introducing noise. 

​	A multivariate linear regression follows exactly the same format as the equation of a line but in higher dimensions. A dimension in this case we consider an attribute of our data. 

**Equation of Multivariate Linear Regression**
$$
f(x)=m_1*x_1+\dots+m_n*x_n+b
$$
$x_1$ through $x_n$ are the attributes of number $n$ we want to consider in our prediction. Continuing from our example above if we want to consider the GPA, Hours studied and whether the student ate breakfast or not we can do so by assigning them to variables. 

| Variable                          | Value                                                 |
| --------------------------------- | ----------------------------------------------------- |
| $x_1=\textbf{GPA}$                | a value between 0.0 and 4.0                           |
| $x_2=\textbf{Hours Studied}$      | a value between 0.0 and 20.0                          |
| $x_3=\textbf{Ate Breakfast}$      | 0 or 1                                                |
| $x_4 = \textbf{Primary Language}$ | 0: for English, 1: for Spanish $\dots$ 100: for Greek |

In this case, ate breakfast is a boolean value, this doesn't affect our prediction model as long as that variable stays within our range.

If we were to visualize a 4d line we would run out of luck, since we have limited number of perceptions to vizualize 4d lines. We will talk more about advanced visualizations techniques and how that would be possible in the future (an easy way to visualize a 4d line is to visualize a 3d line and use color as the 4th dimension)

We can easily visualize **separately** each dependent variable with the indepedent variable we are trying to predict.

```
from sklearn.datasets.samples_generator import make_regression
import matplotlib.pyplot as plt
import numpy as np
reg=make_regression(n_samples=100, n_features=4, noise=20,coef=True,random_state=0)
x=reg[0]
# select first column
x_0=x[:,0]
x_1=x[:,1]
x_2=x[:,2]
x_3=x[:,3]

y=reg[1]
m=reg[2]
x_linespace=np.linspace(-3,3,num=10)

fx_0=m[0]*x_linespace
fx_1=m[1]*x_linespace
fx_2=m[2]*x_linespace
fx_3=m[3]*x_linespace




plt.scatter(x=x_0,y=y)
plt.plot(x_linespace,fx_0, c='r')

# this flushes our current plots and whatever is plotted afterwards will belong to a seperate plot
plt.show() 

plt.scatter(x=x_1,y=y)
plt.plot(x_linespace,fx_1, c='r')
plt.show()

plt.scatter(x=x_2,y=y)
plt.plot(x_linespace,fx_2, c='r')
plt.show()

plt.scatter(x=x_3,y=y)
plt.plot(x_linespace,fx_3, c='r')
plt.show()
```
![image-20190719192528770](../image/image-20190719192528770.png)

![image-20190719192509692](../image/image-20190719192509692.png)

#### In one plot

```
plt.scatter(x=x_0,y=y)
plt.scatter(x=x_1,y=y)
plt.scatter(x=x_2,y=y)
plt.scatter(x=x_3,y=y)
```

For our purposes these plots mean nothing. This is **randomly** generated data only meant to illustrate how we would go about analyzing and visualizing real data.



In a real problem the goal would be to estimate the coefficients $m_1$ through $m_n$ as well as the constant b (y intercept or **bias**)

There is a mathematical solution in calculating all those coefficients directly, however is computationally intensive. Instead we convert this problem into an optimization problem. An optimization problem is a problem for which we iteratevely (like in a loop) we try to find better and better solutions or values for our parameters. We try to find those coefficients in such a way that they minimize a **cost function**. The cost function calculates the current error our line has from our **training data** (the data we use to train our model)

### Example by Python

One way to solve a linear regression is to use our training data and through computationally intensive operations to find the parameters we are trying to estimate. Assume we have (x,y), in this case x is my depedent variable, which is the hours I studied, and y is my indepedent variable, which is the score I got. 

Our solution would be a line of the form $y=m\times x +b$. Solving this dataset by hand we need to perform the following steps. 

1. Calculate $x^2$ for each data point
2. Calculate $x\times y$ for each data point
3. Calculate for the entire dataset the sum. We denote sum of x as $\sum{x}$. We need to find the sum  of $x, x^2, x\times y, \text{ and } y$

The slope $m$ is then given by the equation 
$$
m=\frac{\text{Number of data points}\times \sum{(x\times y)}-\sum{x}\sum{y} }{\text{Number of data points}\times \sum{(x^2)}-(\sum{x})^2 }
$$
The intercept b is then given by the equation
$$
b=\frac{\sum{y}-m\times \sum{x}}{\text{Number of data points}}
$$

```python
from sklearn.datasets.samples_generator import make_regression
import matplotlib.pyplot as plt
import numpy as np

# Number of data points
N=100

# Generate random data points first
reg=make_regression(n_samples=N, n_features=1, noise=20,coef=True,random_state=0)

x=reg[0]
y=reg[1]

# Calculate the square of x (remember because of numpy this operation is applied to individual data point)
x_square=x**2
#similarly, in this case we want to select the first column of x, because of how the data is returned from make_regression
x_y=x[:,0]*y
# .sum at the end of a numpy array calculates the sum
sum_x=x.sum()
sum_y=y.sum()
sum_x_square=x_square.sum()
sum_x_y=x_y.sum()

m_slope=(N*sum_x_y-sum_x*sum_y)/(N*sum_x_square-sum_x**2)
b_intercept=(sum_y-m_slope*sum_x)/N

print("Real slope: %f"%reg[2])
print("Calculated slope: %f"%m_slope)

# we set the bias to 0 in the make regression function by default
print("Real bias (intercept): 0.0")
print("Calculated bias (intercept): %f"%b_intercept)

# We can see our method has some degree of error. Feel free to change the random state to generate different regressions and see that you can't get a perfect answer.
```

> Real slope: 42.385505
>
> Calculated slope: 42.853356
>
> Real bias (intercept): 0.0
>
> Calculated bias (intercept): -1.628364

We can do the same as above in SKLearn using much fewer lines of code, the complete example is below

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets.samples_generator import make_regression

N=100
reg=make_regression(n_samples=N, n_features=1, noise=20,coef=True,random_state=0)

x=reg[0]
y=reg[1]
# Fit function takes our data and fits it into the model finding the correct parameters 
my_model = LinearRegression().fit(x, y)
print("SKLearn LinearRegression slope: %f"%my_model.coef_)
print("SKLearn LinearRegression bias (intercept): %f"%my_model.intercept_)

```

> SKLearn LinearRegression slope: 42.853356
>
> SKLearn LinearRegression bias (intercept): -1.628364

Our results above agree with the SKLearn results

### Training for Supervised Learning

During Machine Learning training, we present an observation to the model we are building and ask it to make a prediction based on that input. We then compare that observation with what the real expected value is. To determine how good our prediction was, we use a **cost function**

#### Cost function

The cost function or **Loss** function takes as input the predicted values that our experimental model is making and using the known ground truth, we can calculate the error of our prediction. The cost function doesn't have a specific form, there are many kinds of functions that are used depending on the problem. We can even define our own to improve training or match our data and problem space. 

​	If I was to use my Exam Predictor model we studied above to determine what my score will be based on how much I studied. 

1. I would start by setting the slope $m$ and intercept $b$ to 0, my initial line would be $y=0\times x + 0$
2. Set input as x (the hours I studied for an exam) and calculate a prediction
3. Compare my prediction to the real score I got in the exam
4. Calculate my error using the **cost function**
5. Update my slope and intercept using an **update rule** 
6. Repeat 2 through 5 until there are no more training samples

The update rule just as the cost function can have different implementations. One of the most basic ones is the Gradient Descent method. 

**Gradient Descent** is an optimization algorithm that tries to minimize the cost function, or our error.

**We will look at gradient descent in more detail in future class**

In python we can use a linear regression function that simplifies calculating the "Ordinary Least Squares" (OLS) method. In reality even this method uses the mathematical closed-form solution (Ordinary Least Squares), instead of Gradient Descent. Gradient Descent is able to solve much more complex linear regression forms, with thousands of attributes for which OLS is unable to handle.



One of the cost functions we can use is called **Mean Squared Error** (MSE). It calculates the distance of our prediction and observation and takes the average of all those distances. 
$$
MSE=\frac{1}{n}\sum_{i=1}^{n}({y_{pred}-y_{real}})^2
$$


If we were to visualize **MSE** it would look something like the following graph:

![](https://uc-r.github.io/public/images/analytics/regression/sq.errors-1.png)

If we sum up the distance of each point to the line and take the average of those distances, we have the exact value of our error (MSE).

**The larger the mean squared error, the worse our model is performing**

A MSE as close to 0 as possible is ideal. But an exact value of 0 is not ideal, that is because it can mean we **overfit** our data, which we will discuss the implications later on. The gist is that a zero MSE means that we pass through every single data point, which might work great for our training data, but will be very far off for new data we haven't seen before. It means we weren't able to capture or find a relationship for our data but instead we tried to fit to every single data point, learning in essense nothing about our data.

MSE is always positive (we are taking the square of the difference of two numbers which is always positive)

**EXAMPLE**

We continue from the example above in calculating our own MSE from scratch and through sklearn.

```python
# make predictions for our training data set 
y_pred=x[:,0]*m_slope+b_intercept

# reg[2] is the coefficient / true slope returned by make_regression
m_slope_true=reg[2]
# this is the ground truth returned by make regression the actual line 0 b intercept
y_ground=x[:,0]*m_slope_true+0


#this is the real y
y

# our own mse
sum_y_y_pred_squared=((y_pred-y)**2).sum()

mse_own=sum_y_y_pred_squared*1.0/N

sum_y_y_ground_squared=((y_ground-y)**2).sum()
mse_ground=1/N*sum_y_y_ground_squared
print("MSE of our generated line: %f"%mse_own)
print("MSE of make_regression line: %f"%mse_ground)
```
We can do the same using sklearn but much simpler, and hopefully the two methods match.

```python
from sklearn.metrics import mean_squared_error
mse_own=mean_squared_error(y_pred, y)
mse_ground=mean_squared_error(y_ground, y)
print("MSE of our generated line: %f"%mse_own)
print("MSE of make_regression line: %f"%mse_ground)
```

```
MSE of our generated line: 332.567565
MSE of make_regression line: 352.351342
```

#### Notes

* Our own method has got a better value for MSE than the ground truth. This is because our data was generated with varying degrees of random noise from a real linear regression line. 
* If we were to add more noise to the data our model would fit even better than the ground truth. Think of the noise as masking the real values of our regression line
* Better MSE than the ground truth line is not because we are doing a better job at describing the data or their relationship. This noise is distracting us from the real relationship. 
* Generating more data and evaluating our model (without fitting it again) it would start performing worse than the ground truth in fitting the data after enough data was collected. 

The above observations are due to the nature of Machine Learning and the world we live in that is stochastic. We can't expect to find a model that describes two variables perfectly, we are only providing estimates and we **try to get as close of a good estimate as possible that can work on data we never seen before.**

### Evaluating our Results

Another important metric by which we can evaluate how good our model is, apart from the mean squared error is the **$R^2$** score (coefficient of determination). The coefficient of determination can assess how well the model can predict future outcomes. It is calculated by the formula: 
$$
R^2=1-\frac{\sum{(y_{true}-y_{pred})^2}}{\sum{(y_{true}-\overline{y_{true}})^2}}
$$
In this case $\overline{y_{true}}$ is the mean value of the ground truth values. 

#### Interpertation

We have a name for the above quantities used in the division. The **total sum of squares** for $\sum{(y_{true}-\overline{y_{true}})^2}$ and **residual sum of squares** for $\sum{(y_{true}-y_{pred})^2}$

**Total sum of squares** can be thought of as the **error** for a model that for every prediction it is asked to make it predicts the mean value of our dataset. 

For example, such a model following from the example above would constantly predict that we would score 50 (the mean value of our dataset) on the exam regardless of how many hours we spend studying for it. 

```python
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
from numpy import random

random.seed(0)

N=10

random_points=random.rand(2,N) # generates coordinates (2) for N points 
x=random_points[0]
y=random_points[1]
plt.scatter(x,y)

# plot the mean line parallel to the x axis, we use min and max for x to plot within the range of numbers we have 
plt.plot([0,1],[y.mean(),y.mean()], c='r')

# this zips like a zipper the two lists and creates a zip object which we convert to list to be able to go over it with the for loop. 
for point in list(zip(x,y)):
  # point is a tuple of the format (x,y) for each coordinate point
  # print(point)
  # this is a quick way to assign a tuple into variables
  p_x,p_y=point
  
  # draw a square from the point to the mean line
  rect_size=abs(y.mean()-p_y)
	# the points of the polygon
  poly_points=[[p_x,p_y],[p_x,y.mean()],[p_x+rect_size,y.mean()],[p_x+rect_size,p_y]]
  rect = Polygon(poly_points, closed=True, fill=True, color='g',alpha=0.5)
  plt.gca().add_patch(rect)

  
# to avoid distortion we set the viewing window
plt.axis((0,1,0,1))

plt.show()
```

![image-20190721103528092](../image/image-20190721103528092.png)

**Notes**

* The red line above is our constant prediction (in this case the mean of all random points)
* The blue points is our dataset
* The green squares is the squared error if were were to make the constant prediction

**Residual sum of squares**

We calculate the same quantity, the square of the difference between the prediction and the ground truth but for our model this time. 

Continuing from the example above, if we were to build a model from the data points of exam score vs study hours, our predictions would try to match as close possible to the ground truth. The square created by points passing through the line of our model, it would be 0, the furthest our model predictions from the ground truth, the higher the area under the square and hence the error will increase. 

```python
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
from numpy import random
import numpy as np

random.seed(0)

N=10

random_points=random.rand(2,N) # generates coordinates (2) for N points 
x=random_points[0]
y=random_points[1]
plt.scatter(x,y)

# we reshape because we want each x feature to be on its own column. The function reshape will help with that
my_model = LinearRegression().fit(x.reshape(-1,1), y)

x_linspace=np.linspace(0,1,num=10)
fx=my_model.coef_*x_linspace+my_model.intercept_
# the c parameter is the color, and in this case we specify red
plt.plot(x_linspace,fx, c='r')


# this zips like a zipper the two lists and creates a zip object which we convert to list to be able to go over it with the for loop. 
for point in list(zip(x,y)):
  # point is a tuple of the format (x,y) for each coordinate point
  # print(point)
  # this is a quick way to assign a tuple into variables
  p_x,p_y=point
  
  # draw a square from the point to the predicted y
  # in this case we reshape the other way around, because we have 1 datapoint we fit into an array.
  # -1 means leave unspecified. More on reshape in future class
  y_pred=my_model.predict(p_x.reshape(1,-1))
  rect_size=abs(y_pred-p_y)
  poly_points=[[p_x,p_y],[p_x,y_pred],[p_x+rect_size,y_pred],[p_x+rect_size,p_y]]
  rect = Polygon(poly_points, closed=True, fill=True, color='g',alpha=0.5)
  plt.gca().add_patch(rect)

  
# to avoid distortion we set the viewing window
plt.axis((0,1,0,1))

plt.show()
```

![image-20190721135523928](../image/image-20190721135523928.png)

Another comparison of the areas (A better picture from wikipedia):

![img](../image/1920px-Coefficient_of_Determination.svg.png)

To calculate $R^2$ we can follow the formula or use the build-in function from sklearn

```python
y_pred=my_model.predict(x.reshape(-1,1))
total_sum_squares=((y.mean()-y)**2).sum()
residual_sum_squares=((y_pred-y)**2).sum()
r_square=1-residual_sum_squares/total_sum_squares
print("R score for random data model")
print("Calculated manually: %f"%(r_square))
print("Calculated by SKLearn: %f"%(my_model.score(x.reshape(-1,1),y)))
```

Our model doesn't perform that well on random data.

**Note**

* **$R^2$ can have a negative value**, this is when our line is arbitarry worse than making a random perdiction. The largest possible value of $R^2$ is 1 (This is when our line goes every single data point without an error)
* $R^2$ is an accuracy metric when evaluating regression models.

### Summary

* Supervised Learning is a technique by which we can build models around data that can help generate predictions for the **indepedent variables ** using only the **depedent variables**. It requires a pair of examples, for the training to occur (indepedent and depedent variable)

* Regression is a method of solving supervised learning problems that are concerned with **indepedent variables** that take **Real** $\R$ , values $(-\infin,\infin)$ e.g. (0.5, 0.52123 as opposed to dog, cat which is a classificataion problem)

* **Linear Regression** builds a model around our data based on the assumption that the data is linearly related. Linearly related data can be described with a single straight line passing through most of them. 
*  **NonLinear Regression** builds a model around data that is not linearly correlated. Techniques on achieving that will be looked in future classes.

* **Multivariate Regression** uses multiple **depedent** variables to predict a single **Indepedent** variable

* **Cost function** calculates the error between our prediction and the **ground truth**. MSE is an example of a cost function

* Gradient Descent vs Close-form, We looked at two techniques for calculating the slope and intercept for a linear regression problem. Close-form in this case is faster and possible but is not possible for all models, in those cases we use Gradient Descent. Both techniques should produce in most cases, equivalent or very close to equivalent solutions.