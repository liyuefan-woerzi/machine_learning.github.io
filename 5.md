# CS82 Introduction to Machine Learning
## Week 5 - Classification
___

## Contents:
 * Classification Algorithms
    - Linear  
    - Non-Linear
 * Linear
    - Logistic Regression
 * Non-Linear
    - Decision Trees   
    - SVM

## Classification

In contrast to regression, for classification we try to predict what class a given data point belongs to instead of what real value it corresponds to.

For a classification problem similar to a regression problem, we are given a set of attributes, $x_1, \dots x_n$ for which we are building a model to make a prediction. In this case our prediction outcome can be a label, for example 0,1,2,3,4. Consider the example of trying to classify given the measurements of a flower's petals what type of flower it is. This is a known open dataset named **Iris** dataset. 

```python
from sklearn import datasets
iris=datasets.load_iris()
X=iris.data
Y=iris.target
plt.scatter(X[:,0],X[:,1],c=Y)
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
```

![image-20190721154651732](../image/image-20190721154651732.png)

We can see the different classes, collored differently. 

To obtain more information about our dataset we can simply execute in jupyter notebook

```python
print(iris.DESCR)
```

> ..
>
> **Data Set Characteristics:**
>
> 
>
> ​    :Number of Instances: 150 (50 in each of three classes)
>
> ​    :Number of Attributes: 4 numeric, predictive attributes and the class
>
> ​    :Attribute Information:
>
> ​        \- sepal length in cm
>
> ​        \- sepal width in cm
>
> ​        \- petal length in cm
>
> ​        \- petal width in cm
>
> ​        \- class:
>
> ​                \- Iris-Setosa
>
> ​                \- Iris-Versicolour
>
> ​                \- Iris-Virginica

You can read the entire description that is very informative, but I have focused on the data set characteristics for now.

#### Toy Dataset

This is a toy dataset. In the previous class we generated our data randomly, this time we are using an existing dataset that is preprocessed and in a really good format for us to evaluate our models on. **Such datasets are not used to solve real world problems.** Instead these datasets serve as a good way to evaluate the performance of our model as well as instructive purposes. You will often come across them as **Toy Datasets**.

### Decision Boundary

For Linear Classification problems we can create a linear boundary that divides the data into their two distinctive classes. The **decision boundary** is a way to express that anything inside the area created by the line (boundary) belongs to a distinct class. For a classification problem we try to divide our data in distinct classes. The classes can be two or more.

Consider the following **Linear Decision Boundary**

![](https://healthcare.ai/wp-content/uploads/2017/08/linear.png)

It is a straight line that separates the red from the blue class. The classification is not perfect, as we also saw with linear regression, there are outliers that do not follow the pattern. However a linear decision boundary on average performs really good.

Consider the following **Nonlinear Decision Boundary**

![](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8materials/ex8a_nofill.png)

The two classes are clearly separable but the boundary that separates them is not a straight line.

#### **Note**

Similarly to regression, a linear classifier can not fit into nonlinearly seperable data. At least not with the techniques we looked in so far, that is without any processing. 

In general a nonlinear classifier can classify linearly separable data, but a linear classifier can not classifier nonlinearly separable data without a transformation.  

In many cases a nonlinear classifier can **overfit** into the data, and can perform worse for linearly classifiable data or not. That is the same problem that can happen if we try to find a line that passes through every data point in a regression problem. We will perform with 100% accuracy on the given dataset but any new data point will be misclassified.

### Linear Classifiers

**Logistic Regression** is a classifier that given a set of attributes returns the class that the given data point belongs to. A Logistic Regression model as the name might imply is in reality an extension of the Linear Regression model. The model calculates a real value and then compares it against the decision boundary which in this case is constant at 0.5. If the value returned by our logistic regression is larger than that value we classify that data point as class 1, otherwise as class 0. 
$$
logit(x)\leq0.5 \text{, Class 1}\\
logit(x)\gt 0.5 \text{, Class 2}
$$
Where $x$ is our data point with its attributes.

Logistic Regression is the **sigmoid function** with a modification to it. 
$$
sig(t)=\frac{1}{1+e^{-t}}
$$
The Sigmoid function has this name because looks like a Sigma or S symbol.
![](https://cdn-images-1.medium.com/max/1200/1*RqXFpiNGwdiKBWyLJc_E7g.png)


We will look at two types of logistic regression, one called  Logistic Regression and another called Multinomial Logistic Regression.

A **Binary Logistic Regression**, is concerned with predicting two classes. For example predicting 0 or 1, the weather is rainy or sunny, and can we pass or fail on our exam. 

A **Multinomial Logistic Regression**, is concerned with predicting more than 2 classes. For example predicting if it is going to be rainy, sunny or cloudy. What letter grade are we going to get in that exam? 

The logistic regression function:
$$
p(x)=\frac{1}{1+e^{-z(x)}}\\
\text{where } z(x)=m_1\times x_1 +\dots+m_n\times x_n+b\\
$$
It is clear that $z(x)$ has exactly the same format as a Linear Regression model. Exactly the same way as a linear regression model the goal of training for a Logistic Regression model is tuning the parameters $m_1 \dots m_n$

The complete model equation is then:
$$
p(x)=\frac{1}{1+e^{-(m_1\times x_1 +\dots+m_n\times x_n+b)}}
$$
The logistic regression function strictly takes values between 0 and 1. Hence it can be thought of as a probability. 

**Plotting Logistic Regression**

```
import numpy as np
import matplotlib.pyplot as plt

def logistic_regression(x):
  return 1.0/(1.0+np.exp(-x))
  
logistic_regression(np.arange(10))  
x=np.linspace(-10,10,num=1000)
y=logistic_regression(x)
plt.plot(x,y)
```
![](../image/30.png)

### How the z(x) affects the decision boundary

For example:

```
import matplotlib.pyplot as plt
b=0
x_1=np.linspace(-5,5,num=1000)
x_2=np.linspace(-5,5,num=1000)
x_3=np.linspace(-5,5,num=1000)
m1=1
m2=1
m3=1
def logistic_regression(x):
  return 1.0/(1.0+np.exp(-x))
  

x=m1*x_1+m2*x_2+m3*x_3+b
y=logistic_regression(x)
plt.plot(x,y)
#decision boundary
plt.plot([np.min(x),np.max(x)],[0.5,0.5], c='red')
plt.axis((-5,5,0,1))

plt.show()

```
![image-20190721164337039](../image/image-20190721164337039.png)



By modifying the b parameter we shift the logistic regression function to left or the right in relation to the decision boundary and depending on the sign of b.
```
b=10
m1=1
m2=1
m3=1
x=m1*x_1+m2*x_2+m3*x_3+b
y=logistic_regression(x)
plt.plot(x,y)
#decision boundary
plt.plot([np.min(x),np.max(x)],[0.5,0.5], c='red')
#plt.axis((-5,5,0,1))

plt.show()


```
![image-20190721165222379](../image/image-20190721165222379.png)

We can see a shift to right. In contrast if we have a negative b we can see a shift to the left.


```
b=-10
m1=1
m2=1
m3=1
x=m1*x_1+m2*x_2+m3*x_3+b
y=logistic_regression(x)
plt.plot(x,y)
#decision boundary
plt.plot([np.min(x),np.max(x)],[0.5,0.5], c='red')
plt.show()
```
![image-20190721165345295](../image/image-20190721165345295.png)

By changing the coefficients we change how *squishy* the logistic function is. Really large values for the coefficients lead to a really sharp curve. 
```
b=0
m1=10
m2=1
m3=1
x=m1*x_1+m2*x_2+m3*x_3+b
y=logistic_regression(x)
plt.plot(x,y)
#decision boundary
plt.plot([np.min(x),np.max(x)],[0.5,0.5], c='red')
plt.show()
```

![image-20190721191131451](../image/image-20190721191131451.png)

Really small coefficient values change the curve and make it more like a straight line
```
b=0
m1=0.1
m2=0.1
m3=0.1
x=m1*x_1+m2*x_2+m3*x_3+b
y=logistic_regression(x)
plt.plot(x,y)
#decision boundary
plt.plot([np.min(x),np.max(x)],[0.5,0.5], c='red')
plt.show()
```

![image-20190721191251059](../image/image-20190721191251059.png)

The goal of the logistic regression similar to a linear regression is to find the coefficients and intercept for which we can minimize our **cost** **function**. 

**Note**

The main takeaway from the above is that instead of changing the value of the decision boundary is sigmoid curve we are changing to adapt to the decision boundary.

### Categorical Cross Entropy

We calculate the performance of our Logistic Regression by using **Categorical Cross Entropy Loss**. This is in contrast to **MSE** loss, which is **only used for regression problems**. For classificataion problems the outcome is a class label (e.g. 0 or 1). Hence the model output is not suitable for a MSE loss.

Categorical Cross Entropy Loss for a single prediction for x
$$
\text{Loss}(p(x))=-\sum_{c=1}^{M}{y_{c}\times ln(p_{c})}
$$


**Notes**

* $ln(x)$ is the natural logarithm it returns a float value. It is answering the question what power do I need to raise the constant $e$ to obtain x. For example  $e^{ln(x)}=x$. We use it because of the range of values it can take

  *  $ln(0)=-\infin$
  *   $ln(1)=0$ 
  *  Rember $0\le p(x) \le1 $

* $y_{c}$ is the ground truth value that is **True** (value of 1) only for the classes that x belongs to and **False** (value of 0) for the classes that x doesn't belong to

* $\sum$ is the sum of all probabilities that the x belongs to. (In most problems only 1 class will be true while all others will contribute 0 to the final loss)

* p(x) is a probability distribution. It is represented by a vector (list) of probabilities that x belongs to a given class. Example
  $$
  [p_{c1}, \dots ,p_{cn}] \text{ e.g.} [0.001,0.02 \dots 0.1]
  $$
  **The sum of p(x) vector should add up to 1.** 

  As an example $p(x)$ can be the function of **logistic regression** that returns the probability that (x) belongs to a class. However other classification models can be used with the Categorical Cross Entropy

**Observations**

* The higher the confidence of the model that x belongs to it's true class, it is penalized less. The confidence of the model for the classes x doesn't belong to is not calculated (since $y_{flag}$ will be zero). 
* The lower the confidence for the class x belongs to, the higher the loss will be. 
* Higher $p_c$ values will cause $ln(p_c)$ to be closer to 0 
* Lower $p_c$ values will cause  $ln(p_c)$ to be closer to negative infinity $-\infin$ (note we multiply by -1 in the equation above so the signs cancel)

**Example**

Let's assume we have two classes, `pass` and `fail` for a given exam. We will encode our classes in a vector called **One-Hot Encoding**, it is a vector that has only zeros, except a value of `1` for the **index** that x belongs to that class. 

For the two classes above our **One-Hot Encoding** vector will have a size of 2. `[0,0]` and we will assign pass to have index `1` and fail to have index `0`. 

* One-Hot Encoding vector for pass `[0,1]`
* One-Hot Encoding vector for fail `[1,0]`

```python
import numpy as np

def categorical_cross_entropy(one_hot,p_x):
	return -(one_hot*np.log(p_x)).sum()

one_hot=[0,1]
p_x=[0.1,0.9]

my_loss=categorical_cross_entropy(one_hot,p_x)

print("categorical_cross_entropy: %f"%my_loss)
```

> categorical_cross_entropy: 0.105361

We can also use the function provided by sklearn

```python
from sklearn.metrics import log_loss

sk_loss=log_loss(one_hot,p_x)
print("log_loss: %f"%sk_loss)
```

> log_loss: 0.105361

We can see our implementations match through our values.

#### Understanding $ln(p_c)$

The main contributing factor to the Loss is the term $ln(p_c)$ since in the equation the other factor is the class flag (a 0 or 1 value). The magnitude of the loss will be determined entirely from $ln(p_c)$. We can give it hypothetical values and plot to see what values it returns.

```python
import numpy as np
import matplotlib.pyplot as plt
# we can't define log of 0
x=np.linspace(0.0001,1,num=100)
y=np.log(x)
ln_x=plt.plot(x,y,label='ln(p(x))',c='b')
n_ln_x=plt.plot(x,-y,label='-ln(p(x))',c='r')
plt.plot([0,1.5],[0,0],c='k')
plt.legend()

plt.show()
```

![image-20190722104812905](../image/image-20190722104812905.png)

The actual value of our loss is the negative of the logarithm, that takes the same values as the red line.

### Binary Logistic Regression

To train a logistic regression for classifying binary outcomes, as we discussed we have to fit the coefficients and intercepts of the linear regression for the sigmoid function. 

Logistic Regression doesn't have a close-form (mathematical) solution but only an optimization solution. That means we can't use mathematics to directly compute the coefficients and we have to come up with another way. 

The training methodology of Logistic Regression is as follows:

1. Make a prediction on the training dataset and obtain the output probability distribution p(x)
2. Calculate the Loss using categorical cross entropy
3. Calculate the derivative of the Loss function (Remember the derivative signifies change and what we are calculating is the rate of change of the error) 
4. Update the new coefficients using update rule that is relative to the derivative of the Loss function (Because we want to minimize the loss we will update the new coefficients to the direction it minimizes such loss)

We will look into more details of that in future classes. For now we can simply use the following code to directly solve a Logistic Regression Problem:

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
x = load_iris()['data']
y = load_iris()['target']
clf = LogisticRegression().fit(x, y)
# creates a prediction from the fited model given our attributes
my_prediction=clf.predict([x[0]])

labels=["sepal length in cm", "sepal width in cm","petal length in cm","petal width in cm"]
print("Predicting the outcome for an iris with characteristics: %s"%dict(zip(labels,x[0])))
print("My Label: %s"%(my_prediction[0]))
print("True Label: %s"%(y[0]))
```


### Multinomial Logistic Regression

Assume we have more than one classes, it is not possible to classify them using a Logistic Regression with only one Decision Boundary. We can achieve that by combining multiple Logistic Regression models that classify into two seperate classes. The two classes in every case would be, our class of interest and then everything else. We then choose the result from the classifier with the highest probability.

**Note**

There are better ways to achieve what is described above, but we won't go into them. However if you feel adventurous, you can look up **Softmax** function.

**Example**

Assume we have classes A, B, C

We build a logistic regression model for classifying A and B&C, B and A&C and C and A&B

![](../image/28.png)

All we have to do then is calculate P(A), P(B) and P( C ) and we take the highest probability among those.

#### Demo

For this demo we will create 3 copies of the same dataset from the iris dataset we looked above. For that reason we will unify/bag together two of the classes in each dataset copy.


```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
X = load_iris()['data']
y = load_iris()['target']

y_0=y.copy()
# get all the y_a not equal to 0 and set them to 1
y_0[y_0!=0]=2
y_0[y_0==0]=1
y_0[y_0!=1]=0


y_1=y.copy()
# get all the y_a not equal to 0 and set them to 1
y_1[y_1!=1]=0

y_2=y.copy()
# get all the y_a not equal to 0 and set them to 1
y_2[y_2!=2]=0
# this line is because we want to keep two classes 0 and 1 and not 0 and 2.
y_2[y_2!=0]=1

# for this demo we will change 
clf_a = LogisticRegression().fit(X, y_0)
clf_b = LogisticRegression().fit(X, y_1)
clf_c = LogisticRegression().fit(X, y_2)

# we set the one datasample into an array because predict always expects multiple samples, we get the 0th element from the results because predict always returns them in an array (since it expects to predict for multiple samples)
index=100
p_a=clf_a.predict_proba([X[index]])[0][1]
# 1 is the class we are interested in. Look at the diagram above if you feel confused.
p_b=clf_b.predict_proba([X[index]])[0][1]
p_c=clf_c.predict_proba([X[index]])[0][1]

print("Probabilities are for beloning to class 0:%s, 1:%s, 2:%s"%(p_a,p_b,p_c))
print("True Label: %s"%y[index])
print("Predicted Label: %s"%np.argmax([p_a,p_b,p_c]))

```

>\>\>\> y_0
>
>array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
>
>\>>> y_1
>
>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
>
>\>>> y_2
>
>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>​       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>
>​       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

#### Results for index 100

> True Label: 2
>
> Predicted Label: 2

#### Note

You don't have to do this every time you want to classify more than one classes. LogisticRegression does this automatically for you (in reality uses a more efficient method).

### Evaluation

To evaluate any classifier similar to the method we used with Linear Regression we can run the `score` function. It returns an accuracy score, that is the ratio of correct to total number of predictions.
$$
Accuracy=\frac{\text{correct predictions}}{\text{total number of predictions}}
$$

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
x = load_iris()['data']
y = load_iris()['target']
clf = LogisticRegression().fit(x, y)
print("accuracy score: %f"%clf.score(x,y))
```

> accuracy score: 0.960000

The accuracy score takes values 0 to 1. Higher accuracy score the better our model performs. There are other metrics we need to consider when evaluating a model such as, **precision** , **recall** which we will look in future lectures.

### Non-Linear Classifiers

A non-linear classifier we will examine in next class is **Decision Trees**, it works by splitting at each feature and deciding based on the most informative feature what class that data point could belong to. 

**Support Vector Machines** is another very popular classifier that performs really well. In its original version is a linear classifier (creates a linear decision boundary) but through a method called **Kernel trick** can be converted to nonlinear classifier.

#### Support Vector Machines (SVM)

By default this classifier is a linear classifier. The goal of this classifier is similar to that of a linear regression, to find a hyperplane. The hyperplane however instead of describing the data it is separating it. The error is calculated such that the hyperplane has the largest margin between the support elemnts.

Take as an example:
![](../image/29.png)

We want to pick a hyperplane boundary that maximizes the margin for the support vectors (which are the vectors or data points closest to the boundary)

It can transform the data points to convert the problem from non-linear into a linear problem. We call such transformations kernels. SVM finds the right kernel to fit the data with and transforms it for us.


```python
from sklearn.datasets import load_iris
from sklearn import svm
x = load_iris()['data']
y = load_iris()['target']
clf = svm.SVC().fit(x, y)
print("accuracy score: %f"%clf.score(x,y))
```

> accuracy score: 0.986667

Which does slightly better than our logistic regression classifier.

Other classifiers can be found in the link: https://scikit-learn.org/stable/supervised_learning.html

A nice visualization comparing some classifiers

![../../_images/sphx_glr_plot_classifier_comparison_001.png](../image/sphx_glr_plot_classifier_comparison_001.png)

### Summary

* Classification is solving a different class of problems than Regression. They both belong in the class of Supervised Learning. 
* Classification predicts labels
* Logistic Regression is a classifier that can be used to classify two or more classes.
* Non-Linear Classifiers can achieve higher accuracy on the same problem when the data is not linearly seperable. But they can also overfit