
# CS82 Introduction to Machine Learning
## Week 7 - Text Classification
___

## Contents:
 * Naive Bayese
 * TF-IDF
 * Tokenizer


### Bayes Theorem

$$
P(A|B)=\frac{P(B|A)\times P(A)}{P(B)}
$$



Bayes Theorem helps calculate the probability of a given Class. It uses the probability of the previous observations to determine using conditional probability what is the probability of the class we are interested in.

A scenario would be trying to predict if an email sent belongs to the class spam or "ham" (Non-spam). Given the subject line of the email has the word "free" such as "Free stuff available click here now!". What is the probability that it is spam and not ham. This is called conditional probability. It is conditional because the email being spam or ham depends on the subject line. So subject line is our given variable.

Conditional probability is denoted as P(A|B), where in this case we are trying to find the probability of event A after we know that event B is true. In our spam and ham case, B can be the word "free". So then we try to find `P(Spam|"Free")`.

The equation is then 
$$
P(Spam|"Free")=\frac{P("Free"|Spam)\times P(Spam)}{P("Free")}
$$


In this case to calculate `P(Spam|"Free")`, we need to know:

* P(B|A) = P("Free"|Spam), the probability of spam emails containing the word "Free"

* P("Free"), the probability of all emails containing the word "Free"

* P(Spam), the probability of all spam emails.


To calculate the probabilities you can look at [Handout 3](3.html). P(A) is the ratio between the number of observations for event A over the total number of observations.

We then have:

`P("Free") = (# of subject lines with word free) / (Total number of emails) `

`P(Spam) = (# of emails we know to be spam) / (Total number of emails) `

`P("Free" | Spam) = (# of emails we know to be spam that contain the word free in the subject line) / (Total number of spam emails) `

### A Practical Example


| | Spam | Ham |
| -------- | -------- | -------- |
| "Free" | 100 |  10 |
| Not "Free" |  |  1000 |

$$
P(Free) = \frac{100+10}{100+10+1000+20}=0.0973
$$

$$
P(Spam) = \frac{100+20}{1000+10+100+20}=0.1062
$$

$$
P(Free|Spam) = \frac{100}{100+20}
$$

Calculating directly from the table:
$$
P(Spam | "Free")=\frac{100}{110}=0.9096
$$


using Bays Theorem
$$
P(Spam | "Free" ) = \frac{100}{100+20}\times\frac{0.1062}{0.0973}=0.9096
$$
So why don't we calculate directly from the observed sample?

This is because Bayes Theorem and for the problems we solve using it we don't know P(A)  the **prior** probability and can change at any given point. This updates our belief about the "world" and the probabilities we try to calculate.

What we try to calculate then is the **posterior** probability 
$$
P(Spam|"Free")
$$

## Naive Bayes

Naive Bayes is a classifier that uses Bayes Theorem to calculate the probability that given a set of features they belong to a given class $C_i$. The Bayes Theorem equation then becomes for a set of features $x=[x_1, x_2, ... , x_n]$ 
$$
P(C_i|\textbf{X})=\frac{P(C_i)\times P(\textbf{X}|C_i)}{P(X)}
$$
So why is Naive Bayes called Naive = "lacking judgement". That is because it makes an assumption in order to calculate the equation above. The assumption is that each probability of each feature is **Independent** of each other feature. 

The equation above using the chain rule is then transformed into:
$$
P(C_i|\textbf{X})=\frac{P(C_i)\times P(x_1,\ldots,x_n,C_i)}{P(x_1,\ldots,x_n)}=\\\frac{P(C_i)\times P(x_1|C_i)\times P(x_\ldots|C_i)\times P(x_n|C_i)}{P(x_1,\ldots,x_n)}
$$
So when deciding which class something belongs to all we have to do is take the maximum $P(C_i|\textbf{X})$

Because $P(X)$ is constant, we are not concerned with calculating it for finding the probability. 

All of this is simplified using SKLearn we can just call:

```python
import numpy as np
from sklearn.naive_bayes import MultinomialNB
X = np.random.randint(10, size=(10, 100))
y = np.random.randint(4, size=10)
clf = MultinomialNB()
clf.fit(X, y)
print(clf.predict(X[0:1]))
```

## Text Classification

Text Classification requires some special handling of the data before we pass it into a classifier. Any classifier is applicable to a text problem as any other problem. However text is extremely unstructured and so far seems unclear on how we can pass it as a feature vector. There are few ways in transforming a text. 

Let's consider a toy dataset

```python
from sklearn.datasets import fetch_20newsgroups
dataset=fetch_20newsgroups()
dataset["data"][0]	
```

> "From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n"

A single instance of the dataset in text classification tasks is called **Document** and the entire dataset is called **Corpus**. A Document can be a tweet, a review, a book, a corpus for those examples could then be tweets about a topic, reviews for hotels, a library. 

How do we convert the above Document into parse-able features? There are a few techniques, namely **Bag of Words**, **TF-IDF**,**N-Gram**, **Stemming**,**POS-Tagging** and more.

### Bag of Words

We can consider a document as a bag of words. We can disregard the connection of words or their position in a document. The objective is to count the frequency of each unique word. We obtain the set of unique words from a corpus. For each document, we seperate words by space, and we count how many occurances we have in that document. That vector, the frequencies of the words is out feature vector.

What happens when a new word appears that we have never had to deal with before when revisiting our model? 

One of the most common things to do is to **ignore unseen words**. We could also include them in our **vocabulary** and update our count table, but that would also require us to update our model (since we now have a feature vector that has 1 additional feature, which is the new unseen word)

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer


dataset=fetch_20newsgroups()
vectorizer = CountVectorizer()
corpus=dataset["data"]
X = vectorizer.fit_transform(corpus)
X.shape

```

`(11314, 130107)`

Very large **sparse** matrix with about 130107 unique words was generated from our corpus

```python
print(dict(zip(vectorizer.get_feature_names(),X.toarray()[0])))
```

![image-20190725152816074](../image/image-20190725152816074.png)

You can observe that many features are empty and even non-sensical. To reduce the size of our unique words we can use techniques to remove uninformative words from the count vectorizer.

### Tokenize

Tokenization is the process of taking documents and breaking it into its constituent elements, the words. There are different techniques that can be applied at this stage to reduce the size of our bag of words matrix.

We can use a **stop word** list, words like and, how, what add no meaning to our classification task. We can ignore them from our matrix.

We can also do other preprocessing such as group together words ignoring their case as well as remove special characters.

If we have a new document we can then transform it to the feature space 

```python
import re

def my_tokenizer(document):
  document=re.sub('[^a-zA-Z]+', ' ', document)
  return document.split(" ")
vectorizer = CountVectorizer(tokenizer=my_tokenizer)

X=vectorizer.fit_transform(corpus)
print(X.shape)
print(dict(zip(vectorizer.get_feature_names(),X.toarray()[0])))
```

Our vocabulary is now `89,000` which is much smaller.

## ![image-20190725154250677](../image/image-20190725154250677.png)

Finding additional conditions or methods to tokenize can help reduce our feature space even further.

### TF-IDF

Having the counts of words in a corpus is not an ideal way to measure how important a word is. This is because for each document we don't have a context in how frequent the word is in the entire corpus. As a result words that are very frequent and not included in our stop word list can overshadow all other words, because of their much higher count. Having a word appear in every document we want to penalize it, for being uninformative. 

TF-IDF calculates a value that helps expres the importance of a word in a document with respect to its frequency in the corpus.

TF-IDF is calculated by multiplying two quantities **TF** = term frequency and **IDF** = inverse document frequency.
$$
TF(word)= \frac{\text{Count of times word is in a single a document}}{\text{Document Size}}
\\
IDF(word)=log\frac{\text{Total number of documents}}{\text{Number of documents with word in it}}
$$
SKLearn can help us calculate TF-IDF using the built-in library. For demonstration purposes I am going to fit the TfIdf vectorizer only on the first document and transform a new sentence.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X_0 = vectorizer.fit_transform([corpus[0]])
print(dict(zip(vectorizer.get_feature_names(),X_0.toarray()[0])))
test_vecttor=vectorizer.transform(["KTbyte is awesome. This is a test sentence and I am wondering if it works!!"]).toarray()
print(dict(zip(vectorizer.get_feature_names(),test_vecttor[0])))

```

> {'15': 0.0, '60s': 0.0, '70s': 0.0, 'addition': 0.0, 'all': 0.0, 'anyone': 0.0, 'be': 0.0, 'body': 0.0, 'bricklin': 0.0, 'brought': 0.0, 'bumper': 0.0, 'by': 0.0, 'called': 0.0, 'can': 0.0, 'car': 0.0, 'college': 0.0, 'could': 0.0, 'day': 0.0, 'door': 0.0, 'doors': 0.0, 'early': 0.0, 'edu': 0.0, 'engine': 0.0, 'enlighten': 0.0, 'from': 0.0, 'front': 0.0, 'funky': 0.0, 'have': 0.0, 'history': 0.0, 'host': 0.0, 'if': 0.35355339059327373, 'il': 0.0, 'in': 0.0, 'info': 0.0, 'is': 0.7071067811865475, 'it': 0.35355339059327373, 'know': 0.0, 'late': 0.0, 'lerxst': 0.0, 'lines': 0.0, 'looked': 0.0, 'looking': 0.0, 'made': 0.0, 'mail': 0.0, 'maryland': 0.0, 'me': 0.0, 'model': 0.0, 'my': 0.0, 'name': 0.0, 'neighborhood': 0.0, 'nntp': 0.0, 'of': 0.0, 'on': 0.0, 'or': 0.0, 'organization': 0.0, 'other': 0.0, 'out': 0.0, 'park': 0.0, 'please': 0.0, 'posting': 0.0, 'production': 0.0, 'rac3': 0.0, 'really': 0.0, 'rest': 0.0, 'saw': 0.0, 'separate': 0.0, 'small': 0.0, 'specs': 0.0, 'sports': 0.0, 'subject': 0.0, 'tellme': 0.0, 'thanks': 0.0, 'the': 0.0, 'there': 0.0, 'thing': 0.0, 'this': 0.35355339059327373, 'to': 0.0, 'umd': 0.0, 'university': 0.0, 'wam': 0.0, 'was': 0.0, 'were': 0.0, 'what': 0.0, 'whatever': 0.0, 'where': 0.0, 'wondering': 0.35355339059327373, 'years': 0.0, 'you': 0.0, 'your': 0.0}

You can see the idf of the sentence `"KTbyte is awesome. This is a test sentence and I am wondering if it works!!"`

`this` has an tf-idf value of 0.35

### N-GRAM

Language requires context in understanding it. The order of words in a sentence contains meaning. Consider the difference between `come to play` and `watched a play` we could misclassify a document because we are missing the difference between the two meanings and are unable to distinguish between the two words that are spelled the same. To combat this, we can create a new feature that is a combination of multiple words. 

For the Example above.

A **2-gram** (after we remove stop words a and to) is `come-play` and `watch-play`

A **3-gram** would include the world following and preceding the words come-play and watch-play. such as come-play-soccer today-come-play and watch-play-hamlet tomorrow-watch-play. The combination of these words captures some meaning about the text and hence are added as a single feature. 

An **n-gram** is the number of words we consider preceding and following each single word in a corpus to create a new feature from them. This technique can increase our feature space exponentially so use it with care. Anything more than 3-gram would be adding noise or have rare applications.

```python
vectorizer=CountVectorizer(ngram_range=(1,3))
X_0 = vectorizer.fit_transform([corpus[0]])
print(vectorizer.get_feature_names())
test_vecttor=vectorizer.transform(["KTbyte is awesome. This is a test sentence and I am wondering if it works!!"]).toarray()
print(dict(zip(vectorizer.get_feature_names(),test_vecttor[0])))
```

> {'15': 0, '15 was': 0, '15 was wondering': 0, '60s': 0, '60s early': 0, '60s early 70s': 0, '70s': 0, '70s it': 0, '70s it was': 0, 'addition': 0, 'addition the': 0, 'addition the front': 0, 'all': 0, 'all know': 0, 'all know if': 0, 'anyone': 0, 'anyone can': 0, 'anyone can tellme': 0, 'anyone out': 0, 'anyone out there': 0, 'be': 0, 'be from': 0, 'be from the': 0, 'body': 0, 'body this': 0, 'body this is': 0, 'bricklin': 0, 'bricklin the': 0, 'bricklin the doors': 0, 'brought': 0, 'brought to': 0, 'brought to you': 0, 'bumper': 0, 'bumper was': 0, 'bumper was separate': 0, 'by': 0, 'by your': 0, 'by your neighborhood': 0, 'called': 0, 'called bricklin': 0, 'called bricklin the': 0, 'can': 0, 'can tellme': 0, 'can tellme model': 0, 'car': 0, 'car is': 0, 'car is made': 0, 'car is this': 0, 'car looked': 0, 'car looked to': 0, 'car please': 0, 'car please mail': 0, 'car saw': 0, 'car saw the': 0, 'college': 0, 'college park': 0, 'college park lines': 0, 'could': 0, 'could enlighten': 0, 'could enlighten me': 0, 'day': 0, 'day it': 0, 'day it was': 0, 'door': 0, 'door sports': 0, 'door sports car': 0, 'doors': 0, 'doors were': 0, 'doors were really': 0, 'early': 0, 'early 70s': 0, 'early 70s it': 0, 'edu': 0, 'edu organization': 0, 'edu organization university': 0, 'edu where': 0, 'edu where my': 0, 'engine': 0, 'engine specs': 0, 'engine specs years': 0, 'enlighten': 0, 'enlighten me': 0, 'enlighten me on': 0, 'from': 0, 'from lerxst': 0, 'from lerxst wam': 0, 'from the': 0, 'from the late': 0, 'from the rest': 0, 'front': 0, 'front bumper': 0, 'front bumper was': 0, 'funky': 0, 'funky looking': 0, 'funky looking car': 0, 'have': 0, 'have on': 0, 'have on this': 0, 'history': 0, 'history or': 0, 'history or whatever': 0, 'host': 0, 'host rac3': 0, 'host rac3 wam': 0, 'if': 1, 'if anyone': 0, 'if anyone can': 0, 'if anyone out': 0, 'il': 0, 'il brought': 0, 'il brought to': 0, 'in': 0, 'in addition': 0, 'in addition the': 0, 'info': 0, 'info you': 0, 'info you have': 0, 'is': 2, 'is all': 0, 'is all know': 0, 'is made': 0, 'is made history': 0, 'is this': 0, 'is this nntp': 0, 'it': 1, 'it was': 0, 'it was called': 0, 'it was door': 0, 'know': 0, 'know if': 0, 'know if anyone': 0, 'late': 0, 'late 60s': 0, 'late 60s early': 0, 'lerxst': 0, 'lerxst wam': 0, 'lerxst wam umd': 0, 'lines': 0, 'lines 15': 0, 'lines 15 was': 0, 'looked': 0, 'looked to': 0, 'looked to be': 0, 'looking': 0, 'looking car': 0, 'looking car please': 0, 'made': 0, 'made history': 0, 'made history or': 0, 'mail': 0, 'mail thanks': 0, 'mail thanks il': 0, 'maryland': 0, 'maryland college': 0, 'maryland college park': 0, 'me': 0, 'me on': 0, 'me on this': 0, 'model': 0, 'model name': 0, 'model name engine': 0, 'my': 0, 'my thing': 0, 'my thing subject': 0, 'name': 0, 'name engine': 0, 'name engine specs': 0, 'neighborhood': 0, 'neighborhood lerxst': 0, 'nntp': 0, 'nntp posting': 0, 'nntp posting host': 0, 'of': 0, 'of maryland': 0, 'of maryland college': 0, 'of production': 0, 'of production where': 0, 'of the': 0, 'of the body': 0, 'on': 0, 'on this': 0, 'on this car': 0, 'on this funky': 0, 'or': 0, 'or whatever': 0, 'or whatever info': 0, 'organization': 0, 'organization university': 0, 'organization university of': 0, 'other': 0, 'other day': 0, 'other day it': 0, 'out': 0, 'out there': 0, 'out there could': 0, 'park': 0, 'park lines': 0, 'park lines 15': 0, 'please': 0, 'please mail': 0, 'please mail thanks': 0, 'posting': 0, 'posting host': 0, 'posting host rac3': 0, 'production': 0, 'production where': 0, 'production where this': 0, 'rac3': 0, 'rac3 wam': 0, 'rac3 wam umd': 0, 'really': 0, 'really small': 0, 'really small in': 0, 'rest': 0, 'rest of': 0, 'rest of the': 0, 'saw': 0, 'saw the': 0, 'saw the other': 0, 'separate': 0, 'separate from': 0, 'separate from the': 0, 'small': 0, 'small in': 0, 'small in addition': 0, 'specs': 0, 'specs years': 0, 'specs years of': 0, 'sports': 0, 'sports car': 0, 'sports car looked': 0, 'subject': 0, 'subject what': 0, 'subject what car': 0, 'tellme': 0, 'tellme model': 0, 'tellme model name': 0, 'thanks': 0, 'thanks il': 0, 'thanks il brought': 0, 'the': 0, 'the body': 0, 'the body this': 0, 'the doors': 0, 'the doors were': 0, 'the front': 0, 'the front bumper': 0, 'the late': 0, 'the late 60s': 0, 'the other': 0, 'the other day': 0, 'the rest': 0, 'the rest of': 0, 'there': 0, 'there could': 0, 'there could enlighten': 0, 'thing': 0, 'thing subject': 0, 'thing subject what': 0, 'this': 1, 'this car': 0, 'this car is': 0, 'this car saw': 0, 'this funky': 0, 'this funky looking': 0, 'this is': 1, 'this is all': 0, 'this nntp': 0, 'this nntp posting': 0, 'to': 0, 'to be': 0, 'to be from': 0, 'to you': 0, 'to you by': 0, 'umd': 0, 'umd edu': 0, 'umd edu organization': 0, 'umd edu where': 0, 'university': 0, 'university of': 0, 'university of maryland': 0, 'wam': 0, 'wam umd': 0, 'wam umd edu': 0, 'was': 0, 'was called': 0, 'was called bricklin': 0, 'was door': 0, 'was door sports': 0, 'was separate': 0, 'was separate from': 0, 'was wondering': 0, 'was wondering if': 0, 'were': 0, 'were really': 0, 'were really small': 0, 'what': 0, 'what car': 0, 'what car is': 0, 'whatever': 0, 'whatever info': 0, 'whatever info you': 0, 'where': 0, 'where my': 0, 'where my thing': 0, 'where this': 0, 'where this car': 0, 'wondering': 1, 'wondering if': 1, 'wondering if anyone': 0, 'years': 0, 'years of': 0, 'years of production': 0, 'you': 0, 'you by': 0, 'you by your': 0, 'you have': 0, 'you have on': 0, 'your': 0, 'your neighborhood': 0, 'your neighborhood lerxst': 0}

Observe how much larger our feature vector is when considering n-grams. 

### Stemming

**Stemming** is the process of taking a word and finding its root word. Example converting played to play or going to go. This is so that we can avoid having multiple features for the same word. Stemming disregards the end of the word if it is part of the root word. The end result might not be an actual word. Consider the example `troubling -> troubl`

**Lemmatization** on the other hand is the same process but returns the actual word for the same example `troubling->trouble`

Stemming and lemmatization require 

We will use NLTK https://www.nltk.org/ for this purpose

```python
from nltk.stem import PorterStemmer
ps = PorterStemmer()
print(ps.stem("going"))
print(ps.stem("troubling"))
```

>go
>
>trouble

As compared to Lemmatizer

```python
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('wordnet')

lm = WordNetLemmatizer()
# we specify that it is the verb format we are interested in
print(lm.lemmatize("going","v"))
print(lm.lemmatize("troubling","v"))
```

> go
>
> trouble

Using with count vectorizer

```
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
analyzer = CountVectorizer().build_analyzer()

def stemmed_words(doc):
    return (stemmer.stem(w) for w in analyzer(doc))

vectorizer = CountVectorizer(analyzer=stemmed_words)
X_0 = vectorizer.fit_transform([corpus[0]])
print(vectorizer.get_feature_names())
test_vecttor=vectorizer.transform(["KTbyte is awesome. This is a test sentence and I am wondering if it works!!"]).toarray()
print(dict(zip(vectorizer.get_feature_names(),test_vecttor[0])))
```

> {'15': 0, '60': 0, '70': 0, 'addit': 0, 'all': 0, 'anyon': 0, 'be': 0, 'bodi': 0, 'bricklin': 0, 'brought': 0, 'bumper': 0, 'by': 0, 'call': 0, 'can': 0, 'car': 0, 'colleg': 0, 'could': 0, 'day': 0, 'door': 0, 'earli': 0, 'edu': 0, 'engin': 0, 'enlighten': 0, 'from': 0, 'front': 0, 'funki': 0, 'have': 0, 'histori': 0, 'host': 0, 'if': 1, 'il': 0, 'in': 0, 'info': 0, 'is': 2, 'it': 1, 'know': 0, 'late': 0, 'lerxst': 0, 'line': 0, 'look': 0, 'made': 0, 'mail': 0, 'maryland': 0, 'me': 0, 'model': 0, 'my': 0, 'name': 0, 'neighborhood': 0, 'nntp': 0, 'of': 0, 'on': 0, 'or': 0, 'organ': 0, 'other': 0, 'out': 0, 'park': 0, 'pleas': 0, 'post': 0, 'product': 0, 'rac3': 0, 'realli': 0, 'rest': 0, 'saw': 0, 'separ': 0, 'small': 0, 'spec': 0, 'sport': 0, 'subject': 0, 'tellm': 0, 'thank': 0, 'the': 0, 'there': 0, 'thi': 1, 'thing': 0, 'to': 0, 'umd': 0, 'univers': 0, 'wa': 0, 'wam': 0, 'were': 0, 'what': 0, 'whatev': 0, 'where': 0, 'wonder': 1, 'year': 0, 'you': 0, 'your': 0}

### Pos Tagging

Part of Speech tagging is a technique by which we assign to each word the most probable part of speech tag. A list of tags and their meaning:

```
    CC coordinating conjunction
    CD cardinal digit
    DT determiner
    EX existential there (like: "there is" ... think of it like "there exists")
    FW foreign word
    IN preposition/subordinating conjunction
    JJ adjective 'big'
    JJR adjective, comparative 'bigger'
    JJS adjective, superlative 'biggest'
    LS list marker 1)
    MD modal could, will
    NN noun, singular 'desk'
    NNS noun plural 'desks'
    NNP proper noun, singular 'Harrison'
    NNPS proper noun, plural 'Americans'
    PDT predeterminer 'all the kids'
    POS possessive ending parent's
    PRP personal pronoun I, he, she
    PRP$ possessive pronoun my, his, hers
    RB adverb very, silently,
    RBR adverb, comparative better
    RBS adverb, superlative best
    RP particle give up
    TO to go 'to' the store.
    UH interjection errrrrrrrm
    VB verb, base form take
    VBD verb, past tense took
    VBG verb, gerund/present participle taking
    VBN verb, past participle taken
    VBP verb, sing. present, non-3d take
    VBZ verb, 3rd person sing. present takes
    WDT wh-determiner which
    WP wh-pronoun who, what
    WP$ possessive wh-pronoun whose
    WRB wh-abverb where, when
```

This task is important because of the difference between `theatrical play` and `play games`

```python
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")

text=nltk.word_tokenize("KTbyte is awesome. This is a test sentence and I am wondering if it works!!")
tagged=nltk.pos_tag(text)
print(tagged)
```

In a similar fashion we can then convert the tags into features. Example trouble-verb is different than trouble-noun and we can create such features in a similar way as n-grams.

```python
from sklearn.feature_extraction.text import CountVectorizer

def my_tokenizer(document):
  tagged=nltk.pos_tag(nltk.word_tokenize(document))
  return ["_".join(pair) for pair in tagged]
 
vectorizer = CountVectorizer(tokenizer=my_tokenizer)


X_0 = vectorizer.fit_transform([corpus[0]])
print(vectorizer.get_feature_names())
test_vecttor=vectorizer.transform(["KTbyte is awesome. This is a test sentence and I am wondering if it works!!"]).toarray()
print(dict(zip(vectorizer.get_feature_names(),test_vecttor[0])))
```

> ```
> {'!_.': 2, "'s_POS": 0, '(_(': 0, ')_)': 0, ',_,': 0, '--_:': 0, '-_:': 0, '._.': 1, '15_CD': 0, '2-door_JJ': 0, '60s/_CD': 0, '70s_CD': 0, ':_:': 0, '?_.': 0, '@_NN': 0, 'a_DT': 1, 'addition_NN': 0, 'all_DT': 0, 'anyone_NN': 0, 'be_VB': 0, 'body_NN': 0, 'bricklin_NN': 0, 'brought_VBD': 0, 'bumper_NN': 0, 'by_IN': 0, 'called_VBN': 0, 'can_MD': 0, 'car_NN': 0, 'college_NN': 0, 'could_MD': 0, 'day_NN': 0, 'doors_NNS': 0, 'e-mail_JJ': 0, 'early_JJ': 0, 'engine_NN': 0, 'enlighten_VB': 0, 'from_IN': 0, 'front_NN': 0, 'funky_NN': 0, 'have_VBP': 0, 'history_NN': 0, 'i_NN': 1, 'if_IN': 1, 'il_NN': 0, 'in_IN': 0, 'info_VBP': 0, 'is_VBZ': 2, 'it_PRP': 1, 'know_VBP': 0, 'late_JJ': 0, 'lerxst_NN': 0, 'lines_NNS': 0, 'looked_VBD': 0, 'looking_VBG': 0, 'made_VBN': 0, 'maryland_NN': 0, 'me_PRP': 0, 'model_NN': 0, 'my_PRP$': 0, 'name_NN': 0, 'neighborhood_NN': 0, 'nntp-posting-host_JJ': 0, 'of_IN': 0, 'on_IN': 0, 'or_CC': 0, 'organization_NN': 0, 'other_JJ': 0, 'out_IN': 0, 'park_NN': 0, 'please_VB': 0, 'production_NN': 0, 'rac3.wam.umd.edu_JJ': 0, 'really_RB': 0, 'rest_NN': 0, 'saw_VBD': 0, 'separate_JJ': 0, 'small_JJ': 0, 'specs_NN': 0, 'sports_NNS': 0, 'subject_NN': 0, 'tellme_VB': 0, 'thanks_NNS': 0, 'the_DT': 0, 'there_RB': 0, 'thing_NN': 0, 'this_DT': 1, 'to_TO': 0, 'university_NN': 0, 'wam.umd.edu_NN': 0, 'was_VBD': 0, 'were_VBD': 0, 'what_WP': 0, 'whatever_WDT': 0, 'where_WRB': 0, 'wondering_VBG': 1, 'years_NNS': 0, 'you_PRP': 0, 'your_PRP$': 0}
> ```

With a pos tagger the same word can appear multiple times as a feature with the different POS that it accompanies it. For example `play_VB` and `play_NN`

**Chunking** is the process by which we create a relationship tree of the POS Tagged elements. We want then to combine the elements such that a verb is with its noun. This helps in extracting more meaning from the sentence. 

```python
grammar = ('''
    NP: {<DT>?<JJ>*<NN>} # NP
    ''')
chunkParser = nltk.RegexpParser(grammar)
tagged=nltk.pos_tag(nltk.word_tokenize("There is a difference between playing soccer and watching a play."))


tree = chunkParser.parse(tagged)
tree.draw()
```

![image-20190725193308315](../image/image-20190725193308315.png)

### Named-Entity Recognition

Consider we are talking about Apple, is it the apple we eat or the company? In some cases such as sentiment analysis we need a distinction. For this reason we have a specific task called name-entity recognition. 

**spaCy** is another useful package http://spacy.io that helps with text classification. It has a build in database of named entities and depending on the text and the POS tag it can recognize the difference between apple and Apple.  

```
!pip3 install spacy
!python3 -m spacy download en_core_web_sm
```

```python
import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()

doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')


for token in doc:

    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,

            token.shape_, token.is_alpha, token.is_stop)
```



### 20 news groups

```
from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

dataset=fetch_20newsgroups()

count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(dataset.data)

tfidf_transformer = TfidfTransformer()

X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
clf = MultinomialNB().fit(X_train_tfidf, dataset.target)
clf.score(X_train_tfidf, dataset.target)
```

> 0.93

### Cross Validation

Using the score function on our train dataset is not correct methodology. This is because we are training and testing on exactly the same dataset. Cross Validation is a technique that aims to evaluate the real performance of our model. 

The first and most important thing is splitting the dataset into training and testing. This is so that we train on one dataset and then test on another dataset that the model has never seen before. This will help us evaluate how well the same model will perform with other unseen data.

We can split the dataset as follows

```
from sklearn.model_selection import train_test_split

from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer


dataset=fetch_20newsgroups()

X_train, X_test, y_train, y_test=train_test_split(dataset.data, dataset.target, test_size=0.3, random_state=0)



count_vect = CountVectorizer().fit(X_train)

X_train_counts = count_vect.transform(X_train)

tfidf_transformer = TfidfTransformer()

X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
clf = MultinomialNB().fit(X_train_tfidf, y_train)

X_test_counts=count_vect.transform(X_test)
X_test_tfidf=tfidf_transformer.transform(X_test_counts)

clf.score(X_test_tfidf, y_test)


```

>  0.84

As it was expected a much lower accuracy score.

### KFold

Because splitting can also have a random chance, and by splitting the data at different points we can obtain completely different accuracies. To better evaluate our model we need to validate that it wasn't just a lucky or unlucky split that was responsible for the score we obtained. 

**KFold** is a cross-validation technique that splits the data multiple times and uses different train and test samples to train and obtain an average score. 

![Image result for kfold](../image/The-technique-of-KFold-cross-validation-illustrated-here-for-the-case-K-4-involves.png)

**Example**

```python
from sklearn.model_selection import train_test_split

from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import KFold

import numpy as np

dataset=fetch_20newsgroups()


kf = KFold(n_splits=2)
X=np.array(dataset.data)
y=dataset.target
scores=[]
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    count_vect = CountVectorizer().fit(X_train)
    X_train_counts = count_vect.transform(X_train)
    tfidf_transformer = TfidfTransformer()
    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
    clf = MultinomialNB().fit(X_train_tfidf, y_train)
    X_test_counts=count_vect.transform(X_test)
    X_test_tfidf=tfidf_transformer.transform(X_test_counts)
    scores.append(clf.score(X_test_tfidf, y_test))
print(np.mean(scores))
```

> 0.81

Which is even worse than before. However we are guranteed accuracy scores very close to this with this cross validation method every time we would run the above code.

#### Number of splits

The most common value for number of splits is 10. However depends on your dataset size as well. Cross-validation takes time, so it is used after we feel confident in our model or to try different training parameters for our model. 