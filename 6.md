#  CS82 Introduction to Machine Learning

## Week 6 - Decision Trees and Feature Engineering
___

## Contents:
 * Introduction to Feature Preprocessing
 * Scaling of Features
 * Encoding
 * Discretization
 * Feature Importance

### Features

Features or attributes of data in the real world are often unprocessed. They often come bundled together and might have no use in solving the problem. In fact uneccessary features can add noise and make our model worse. 

In other cases there is hidden information into our feature that maybe we are better at understanding and examining by visualising the data than by reading through it. 

The linear vs non-linearity problem is an example of how we can visualize data and find that there is a pattern but our linear classifier finds it impossible classify. We will look at such example below.

**Feature Engineering** 

Consider the artificial dataset below. We generate datapoints using the `make_circle` build-in function from sklearn that generates points that follow a circular pattern.

```python
from sklearn.datasets import make_circles
circles=make_circles()
X=circles[0]
y=circles[1]
plt.scatter(X[y==0][:,0],X[y==0][:,1],c="r")
plt.scatter(X[y==1][:,0],X[y==1][:,1],c="b")
plt.show()
```

![image-20190722233408875](../image/image-20190722233408875.png)

One way to convert the points of the circle into data that we can linearly classify is by using **Feature Engineering**. Feature Engineering, takes existing features and creates new ones or replaces the old ones with those new features. This takes away some or a lot of the friction from the classifier/regressor trying to learn a relationship from the data. 

It is also impossible sometimes for our classifier/regressor to learn such relationships.

The idea is that as humans we are domain experts of the data and we can understand it really well. We can then see patterns in the data and we can bring them to the surface and make them more accessible to our model. Doing so increases the performance of our model.

**Example**

Continuing from the example above. We have two features in this artificial dataset, $x_1$ and $x_2$, We can convert them from polar coordinates (circle) to cartesian system. It is a mathematical transformation that will help us seperate the data.

```python
from sklearn.datasets import make_circles
import numpy as np
circles=make_circles()
X=circles[0]
y=circles[1]

x_1=X[:,0]
x_2=X[:,1]

# the equation of a circle r^2=x_1^2+x_2^2
r=np.sqrt(x_1**2+x_2**2)

theta=np.arctan(x_2/x_1)
plt.scatter(r[y==0], theta[y==0], c="red" ,label='Red')
plt.scatter(r[y==1], theta[y==1], c="b" ,label='Blue')
plt.legend()
plt.show()
```

![image-20190722234643346](../image/image-20190722234643346.png)



We have then converted a non-linear classification problem to a linear classification problem. Our new features are $r$ and $\theta$

If you are having trouble understanding Polar Coordinates I suggest you read: http://tutorial.math.lamar.edu/Classes/CalcII/PolarCoordinates.aspx

We can try to compare the classification between the two datasets on a logistic regression and evaluate their performance

```python
from sklearn.linear_model import LogisticRegression

clf=LogisticRegression().fit(X,y)
print("Classification accuracy for non-engineered dataset: %f"%clf.score(X,y))
X_engineered=np.array(list(zip(r,theta)))
clf_engineered=LogisticRegression().fit(X_engineered,y)
print("Classification accuracy for engineered dataset: %f"%clf_engineered.score(X_engineered,y))
```

>Classification accuracy for non-engineered dataset: 0.500000
>
>Classification accuracy for engineered dataset: 1.000000

The model trained on the non-engineered dataset, does as well as if it was to do a random prediction (50% accuracy is really low)

A non-linear classifier would outperform our model on non-engineered data because it would be able to extract the relationship we build into the data ourselves. 

```python
from sklearn import svm
clf = svm.SVC().fit(X, y)
print("accuracy score: %f"%clf.score(X,y))
```

> accuracy score: 1.000000

### Types of Features

Features are the attributes that we use to make a prediction. They can have different formats but we can't always use the format they are in. Many times we will have to modify it in order to be able to use it as part of our machine learning model.

For now we will divide features into two categories, Numerical and Categorical.

**Numerical** features are numbers that represent a measurement. 

​	**Example** we measure the temperature, SAT score etc. However not all numbers are numerical values.

​	We can have two types of Numerical Values, Continuous and Discrete.

* **Continuos Features** are numerical values that are float numbers, they can be representing any real number e.g. 4.2911

* **Discrete Features** are numerical values that are integer values, they can be representing any countable or discrete entity. e.g. 1, 5882

**Categorical** features are values that represent items inside a group. A group is a fixed size set of possible values. 

​	**Example** Gender, Weather (Sunny, Rainy, Overcast etc).

​	We can have two types of Categorical features Ordinal and Nominal.

* **Ordinal** features are categorical features that can be **Ordered**. Example: A feature from a survey that ranks a persons happiness "How happy do you feel? (1 - 10)"

* **Nominal** features are categorical features that do not have order encoded in them. Example: What is your most favorite food? Pizza, Hot Dog etc.

#### More

There are other types of data as well, such as images, text, time series and text.

**Notes**

Ordinal numbers take values the same way as numerical features but they are different. Ordinal values have no concept of average, addition, subtraction. We can't do operations in Ordinal values. We only know the order, we don't know the magnitude of their difference. 

Example we know someone answering 8 in the happy scale is happier than someone answering 5 but we **can't** measure that the first person is 3 points happier than 5 because the measure is **subjective**, All we know is that person A is happier than person B. The scale by which they are happier could for example be logarithmic and not linear (so 1 point of happiness difference is much more in magnitude at the lower end of the scale)

### Wine Dataset

Wine Dataset is a real world dataset that includes tweets from users that rated different varieties of wine, from the WineEnthusiast magazine during the week of June 15th, 2017.

We can explore our data

```python
import pandas as pd

wine_dataset=pd.read_csv("https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.csv")
display(wine_dataset[["points","price","country"]])
```

![image-20190723092028125](../image/image-20190723092028125.png)

We can identify that, Price is a continuous feature, Country is a Nominal feature and points is an Ordinal feature.

Another thing we can observe is missing values. For the first entry (with index 0) the price is `NaN`. That is very common with real world datasets. The problem we NaN values is that we can't run mathematical operations on the data. We can't obtain the mean, because we don't know how to handle missing data, consider it a 0? ignore it? There is no right answer and highly depends on your dataset

### Dealing with NaN values

The simplest and easiest thing we can do is **remove the data points that contain them**. `wine_dataset_clean=wine_dataset.dropna()` or `wine_dataset_clean=wine_dataset.dropna(subset=['price'])` . However this is going to be problematic if we have multidimensional (many features) data and at least one feature is missing from each data point. This will lead us to an empty dataset. 

We can also **ignore the entire attribute** that contains `NaN` values if it is not very important for building our model. 

**Set the `NaN` value as the mean for that attribute**. This is not a great idea but many times in works in aproximating what the real missing value could be. This works well with numerical data. In the example above we could have done. `wine_dataset_clean.loc[pd.isna(wine_dataset_clean['price']),'price']=wine_dataset_clean['price'].mean()`

**Interpolating missing values** , this technique uses values that are surounding our missing value to fill in the blanks. There are a few types of interpolations we can do. **Linear Interpolation**, fill in the blank assuming that there is a linear relationship to the data and assume it is equally spaced. **Time interpolation** for unevenly spaced data points that we have a context of time. **Nearest, cubic, polynomial and more** when the data has more complex relationships then we can use more specific methods. 

**Note** Interpolation requires that we have some order to our data, either column wise or row wise. It also requires that we understand some fundamental relationship about our data to fill in the gaps.

**Example** We have tempratures over a given 24 hour period for an industrial oven, we know that the variations in temprature don't differ by more than few degrees between any given hour. Assume that our measuring device broke down for 4 hours and we have 4 missing data points. We can interpolate from the tempratures before and after and create 4 artificial data points. 

```python
import pandas as pd
# temprature in celcious for a 24 hour period in an industrial oven
temprature = pd.Series([20.61, 20.68 , np.nan,np.nan,np.nan,np.nan, 22.81, 24.13, 24.56, 25.31, 25.02, 25.33, 25.39, 26.59, 26.44, 26.90, 27.03, 27.21, 27.93, 27.97, 28.84, 28.75, 29.86, 29.44 ])

# default method linear
temprature.interpolate()
```

> [20.61, 20.68, 21.105999999999998, 21.532, 21.958, 22.384, 22.81, 24.13, 24.56, 25.31, 25.02, 25.33, 25.39, 26.59, 26.44, 26.9, 27.03, 27.21, 27.93, 27.97, 28.84, 28.75, 29.86, 29.44]

The guesses for a linear interpolation were `21.11, 21.53, 21.96, 22.38`. While we do expect the temprature to be a  can compare those estimates with other interpolation techniques. 

```python
temprature.interpolate(method='nearest')
# [20.61, 20.68, 20.68, 20.68, 22.81, 22.81, 22.81, 24.13, 24.56, 25.31, 25.02, 25.33, 25.39, 26.59, 26.44, 26.9, 27.03, 27.21, 27.93, 27.97, 28.84, 28.75, 29.86, 29.44]
temprature.interpolate(method='quadratic')
#[20.61, 20.68, 20.77644468774998, 20.89933406324993, 21.104001313849864, 21.722445563649885, 22.81, 24.13, 24.56, 25.31, 25.02, 25.33, 25.39, 26.59, 26.44, 26.9, 27.03, 27.21, 27.93, 27.97, 28.84, 28.75, 29.86, 29.44]
```

#### Wine Dataset cleaning

We can't interpolate for the wine dataset because we don't have an ordered index for price. If for example we knew that points were highly correlated with the price of the wine we could use points column as an index and interpolate the missing data on the price. However, price and points are not highly corelated and we can't do that. 

The problem I have decided to solve with this dataset is to predict the price of a wine based on the remaining available attributes (such as origin, points awarded on average). For this reason I can't consider wines that have no price available and I will have to drop any data points that don't have a price tag. 

 ```python
import pandas as pd

wine_dataset=pd.read_csv("https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.csv")

wine_dataset_clean=wine_dataset.dropna(subset=['price'])

 ```

### Scaling of Features

Features can take arbitary ranges in values, scaling those features within a range can mainly improve the computational cost of training our model. In addition to that, some algorithms use the Euclidean distance between data points and unscaled features increase the importance of a specific attribute to that specific model. Some models for which scaling is required:

* K-Means 
* K-NN
* PCA
* GD


### MinMaxScaler

Converts features to 0-1 range using the formula
$$
\text{MinMaxScaler(x)}=\frac{x-min(x)}{max(x)-min(x)}
$$

```
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import numpy as np

scaler = MinMaxScaler()

wine_scaled=wine_dataset_clean[['points','price']].copy()
points=np.array(wine_scaled['points'])
points_min_max=scaler.fit_transform(points.reshape(-1,1))
plt.hist(points,bins=100)
plt.show()

plt.hist(points_min_max,bins=100)
plt.show()
```

![image-20190725093034633](../image/image-20190725093034633.png)

We can observe that the range is between 0 and 1.

#### fit_transform

The function `fit_transform` is a two step process. It first `fit` the data and then `transform` the data. It does the fitting of the data and transformation at one step. In the future if we collect more data we would want to also scale it within the same range. If we don't have access to the original data to rescale them or we don't have the ability to retrain from scratch our model what we want to do is keep a copy of the scaler. 

The above example is equivalent

```
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import numpy as np

scaler = MinMaxScaler()

wine_scaled=wine_dataset_clean[['points','price']].copy()
points=np.array(wine_scaled['points'])
random_points=np.random.uniform(low=0,high=1000,size=100)


my_scaler=scaler.fit(points.reshape(-1,1))
wine_points_min_max=my_scaler.transform(points.reshape(-1,1))
random_points_min_max=my_scaler.transform(random_points.reshape(-1,1))

plt.hist(wine_points_min_max,bins=100)
plt.title("Wine Dataset")
plt.show()

plt.hist(random_points_min_max,bins=100)
plt.title("Random Uniform points in range 0 to 1000")
plt.show()


```

![image-20190725095819795](../image/image-20190725095819795.png)

The second transformation even includes negative values because the minimum of the random distributed points was less than the original data it was fitted on. **It is not correct to fit a different scaler for the same attribute when training the same model**. This is because it can lead to incosistencies in the range as pointed out above and they will make your model perform poorly. 

### Normalization

It is scaled such that the norm of the vector for each data point is 1. This procedure is especially useful for text based problems. This is because we perform vector based operations on such problems for the data features because it provides additional mathematical relationships that can be run on the data (such as cosine similarity).

The norm of a vector is its Euclidean distance from the origin
$$
x=
\begin{bmatrix}x_1 \\ \dots \\x_n \end{bmatrix}\\
|x|=\sqrt{x_1^2+\dots+x_n^2}
$$
Our scaling makes it so that $|x|=1$

```python
from sklearn.preprocessing import Normalizer
import matplotlib.pyplot as plt
import numpy as np


wine_scaled=wine_dataset_clean[['points','price']].copy()
points=np.array(wine_scaled['points'])

norm = Normalizer()
# Please note that we treat our entire attribute as a single data point for demo purposes. In all other cases you would input your dataset normally to this function
points_norm=norm.fit_transform([points])

plt.hist(points,bins=100)
plt.show()

plt.hist(points_norm.flatten(),bins=100)
plt.show()

l2_norm=np.sqrt((points_norm**2).sum())
print(l2_norm)
```

> 0.9999999

More scalers are available depending on your use-case, you can read more about them: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html

![image-20190725093140598](../image/image-20190725093140598.png)

We can observe that the range is between 0 and 0.0032

### Non-linear Transformation

Scaling is a linear transformation. The distribution of the data doesn't change but only the range of their values.  Non-linear transformations on the other hand can change the distribution of the data points to fit a specific distribution. There are different ways to transform data and each method has its positives and negatives. The main reason you would want to transform the data is because your model is making some assumption such as that the data is normally distributed (the bell curve). Many models (especially the ones with the word gaussian in them) expect your data to be normally distributed. In such cases you would want to perform a non-linear transformation. 

Our wine rating data appears to be normally distributed with some noise build into it. We might want to correct for that noise by further transforming the point range to better fit a bell-curve. 

```
from sklearn.preprocessing import QuantileTransformer
qt = QuantileTransformer(output_distribution='normal')
points=np.array(wine_scaled['points'])
normal_wine_points=qt.fit_transform(points.reshape(-1,1))
plt.hist(points,bins=100)
plt.show()
plt.hist(normal_wine_points,bins=100)
plt.show()
```

![image-20190725110011415](../image/image-20190725110011415.png)

In this case our data didn't change much since it was already very close to a normal distribution curve. One thing to note is that the mean is at 0 and our range now includes negative values. For our wine point example perhaps negative values would not work that well.

### Encoding

**Nominal** values can be specifically difficult to work with. We can't input `pizza` into a formula. We have to find a way to convert nominal values which in most cases are text into an encoded representation that we can work with in our model. There are two main approaches at encoding our data, a LabelEncoder and a OneHotEncoding. In general you will rarely benefit from a LabelEncoder and you would always want to use a OneHotEncoder. 

#### LabelEncoder

A label encoder accepts as input a dataset for which there are multiple labels in the feature space and converts them to integer representation. Moreover the label encoder keeps a record of the mapping between a label and its encoded value.

#### Example

Assume I want to encode the country of origin for a wine to use it on a model I am building. I can build a label encoder for the 



```
from sklearn import preprocessing

le=preprocessing.LabelEncoder()
wine_dataset_clean=wine_dataset.dropna(subset=['country'])


X=le.fit_transform(wine_dataset_clean[['country']])
print(le.classes_)
print(X)
print(le.inverse_transform([0]))
print(le.transform(['Australia']))
```

#### OneHotEncoding

One hot encoder takes a list of labels and creates a matrix from them. 

* Each **column** in the matrix represents a label
* Each **row** is a data point
* Each **cell** is a **boolean value**, it is true only for the label that the data point belongs to. 

#### Example

Assume we have labels for country of origin of our wine dataset as `['US','Argentina','Greece','Italy']` 

We have 10 data points with the following attributes

| Index | Country of Origin | Price |
| ----- | ----------------- | ----- |
| 0     | US                | 10    |
| 1     | Argentina         | 14    |
| 2     | Greece            | 13    |
| 3     | Italy             | 14    |
| 4     | Greece            | 12    |
| 5     | US                | 9     |
| 6     | Italy             | 7     |
| 7     | Italy             | 23    |
| 8     | Argentina         | 12    |
| 9     | Italy             | 15    |

A one hot encoded matrix will then look like the following

|      | US   | Argentina | Greece | Italy | Price |
| ---- | ---- | --------- | ------ | ----- | ----- |
| 0    | 1    | 0         | 0      | 0     | 10    |
| 1    | 0    | 1         | 0      | 0     | 14    |
| 2    | 0    | 0         | 1      | 0     | 13    |
| 3    | 0    | 0         | 0      | 1     | 14    |
| 4    | 0    | 0         | 1      | 0     | 12    |
| 5    | 1    | 0         | 0      | 0     | 9     |
| 6    | 0    | 0         | 0      | 1     | 7     |
| 7    | 0    | 0         | 0      | 1     | 23    |
| 8    | 0    | 1         | 0      | 0     | 12    |
| 9    | 0    | 0         | 0      | 1     | 15    |

The advantage of this technique as compared to a label encoder is that we don't assign a **numeric value** to a Country because that would make the implication that we can perform mathematical operations with that Country. So the average of country would be Italy, or Italy is larger than Argentina. Our model will make such assumptions and when the value of that country will be used during training our classification it will perform poorly because it will add bias. Hence it is **always** better to onehotencode as compared to use a label encoder. 

One **disadvantage** that we would want to consider a label encoder instead is when we have too many distinct labels. In the example above we only had 4 distinct labels and our resulting one hot encoding matrix was `4xN`. If we had thousands of distinct labels this would increase the size of our feature space to thousand distinct features. This would cause training to be extremely difficult and sometimes impossible all together. In such cases we would want to consider a label encoder. 

```
from sklearn import preprocessing
from sklearn.model_selection import train_test_split


oe=preprocessing.OneHotEncoder()
wine_dataset_clean=wine_dataset.dropna(subset=['country'])


X=oe.fit_transform([wine_dataset_clean['country']])
# X is a sparse matrix which is a format meant to conserve memory space
country_one_hot=pd.DataFrame(X.toarray())
country_one_hot.columns=oe.get_feature_names()
display(country_one_hot)
```

### Feature Selection

#### Correlation

Correlation is the statistical relationship between two variables. Pearson Correlation coefficient is a value between -1 and 1. It is a widely used metric for exploring the relationship between two random variables. 

* A correlation value of 0 indicates that there is no connection between the two variables. That means that the variables are completely indepedent of each other, and the change in value of either of those variables will not affect the other. Example, the price of a wine with the current temperature are 2 indepedent variables. 
* A negative correlation indecates an inverse relationship between two variables, the increase/decrease on the value of one of the variables has an inverse relationship with the increase/decrease of the other variable. The closer the coefficient to -1, the larger the extend. Example, the price of a wine with the number of available wines in the market. More wines could mean more competition and lower prices for a given bottle of wine.
* A positive correlation indicates a direct relationship between two variables, the increase/decrease on the value of one variable has a direct relationship with the increase/decrease of the other.Example, the price of the wine with age of the wine, the higher age of the wine, the higher the price for that given bottle of wine will be.

![Image result for negative correlation examples](../image/correlation-examples.svg)

Correlation does not imply causation. That means that because two things are correlated doesn't always mean that they are really correlated and we can't draw conclusions with what we have seen in this course so far about whether or not either of those variables causes the other's value to change. Example, the thermometer values are highly correlated with the weather. However we can't say that changes in the thermometer's values cause the weather temprature to change. In statistics and with what we have seen so far we also can't be sure that the changes in weather are what causes the thermometer level to change. 

```
import matplotlib.pyplot as plt
import pandas as pd

wine_dataset=pd.read_csv("https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.csv")

wine_dataset_clean=wine_dataset.dropna(subset=['price'])


corr=wine_dataset_clean[['points','price']].corr()
plt.matshow(corr)
plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical');
plt.yticks(range(len(corr.columns)), corr.columns);
display(corr)
plt.show()
```

![image-20190725134020845](../image/image-20190725134020845.png)



We can observe that the correlation between price and points is there but is not that great.

### Feature Selection

Feature selection is the active process by which we consult different statistical analysis metrics in choosing the attributes we want to consider for our predictive model. We never want to consider all attributes to our model because some of them might add noise.

We can select which attributes to choose by measuring the correlation between the indepedent variable (what we are trying to predict) and the depedent variables. If a depedent variable has a very low correlation with our indepedent variable, we might want to exclude it from our model.

Another way is to select the attributes that add the most information in our model. **Information Gain** is a metric which measures how informative a variable is. There are algorithms such as **Decision Trees** which construct a tree using Information Gain as a metric to decrease the size of the branches. At each level in the tree they choose which attribute to split at by choosing the one with the highest **Information Gain**


### Decission Trees

![](https://cdn-images-1.medium.com/max/1200/0*Yclq0kqMAwCQcIV_.jpg)



#### Entropy

Entropy is the measure of disorder. Higher values means higher disorder. 

What is the best split among the data?

Entropy helps calculate how much information we gain on each split. There are many ways to build our tree, but what is the most efficient Tree?

We split on datapoints that are more **important** in classifying first.

>> Am I hungry?

If we checked if we had 25$ first we would have to check if we are hungry twice. Not very efficient.

From Class 5 we can remember the equation for Entropy that was also used as a loss function.

Formula for Entropy for a single attribute
$$
\text{Entropy}=\sum_{i}{-p_i\times \log_2{p_i}}
$$
In this case we calculte the exact probability of each class by measuring their probabilities. 

* $p_i$ is the probability distribution of class i

$$
\text{Information Gain}= Entropy(Parent)-[\text{average entropy(children)}]
$$

We want maximize Information Gain at each split.

#### Visual Explanation

![img](../image/1*zMu0UClotNXljrjqmyRIHA.png)

We choose to split our dataset for when balance is `50k`. As you can see the Entropy is minimized at each split (less disorder, less mixture of different classes).

#### Example

```python
from sklearn.datasets import load_iris
from sklearn.tree  import  DecisionTreeClassifier

iris = load_iris()
clf = DecisionTreeClassifier()
clf=clf.fit(iris.data,iris.target)
print("Accuracy Score: %f"%clf.score(iris.data,iris.target))
```

> Accuracy Score: 1.0

#### Feature Importance

```
feature_importance=dict(zip(iris.feature_names,clf.feature_importances_))
plt.bar(range(len(feature_importance)), list(feature_importance.values()))
plt.xticks(range(len(feature_importance)), list(feature_importance.keys()), rotation=90)
plt.show()
```
![image-20190725145731282](../image/image-20190725145731282.png)

#### DecisionTreeRegressor

Similar to the classifier above we can use the same for a regression problem such as the wine example above.

```python
from sklearn.datasets import load_iris
from sklearn.tree  import  DecisionTreeRegressor
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder

wine_dataset=pd.read_csv("https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.csv")

clf = DecisionTreeRegressor()
columns_of_interest=['country', 'designation', 'points', 'variety', 'winery']
wine_dataset_clean=wine_dataset.dropna(subset=['price'])
X=wine_dataset_clean[columns_of_interest].fillna('')
from sklearn.preprocessing import LabelEncoder
# We are not using one hot encoding because we have too many labels. Also doesn't help in determining which feature is most informative since we will obtain the one hot encoded version of that attribute.
X['country']=LabelEncoder().fit_transform(X['country'])

X['designation']=LabelEncoder().fit_transform(X['designation'])
X['variety']=LabelEncoder().fit_transform(X['variety'])
X['winery']=LabelEncoder().fit_transform(X['winery'])

clf=clf.fit(X,wine_dataset_clean['price'])

feature_importance=dict(zip(columns_of_interest,clf.feature_importances_))
plt.bar(range(len(feature_importance)), list(feature_importance.values()))
plt.xticks(range(len(feature_importance)), list(feature_importance.keys()), rotation=90)
plt.show()
```

![image-20190725151429157](../image/image-20190725151429157.png)

We can see that winery is the most informative feature in determining the price of a wine while the least informative feature is country of origin. In this case we might want to drop the country attribute in our classification task