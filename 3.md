# CS82 Introduction to Machine Learning

## Week 3 - Linear Algebra
## Linear Algebra

Linear Algebra is a tool by which we can manipulate large groups of numbers by defining single operations to them rather than distinct calculations of each of the elements of those groups. Such operations make expressing and writing complex mathematical concepts fairly straightforward. It help us abstract more complex concepts and make working with large numbers more intuitive. 

A Group of numbers in the case of Linear Algebra is an array, that array/list can have multiple dimensions (1-D array, 2-D, 3-D, 4-D or more). We define operations between those arrays, such as how to add, substract, multiply and divide between those groups of numbers. 

**How is Linear Algebra Used in Machine Learning?** 

A lot of the Linear Algebra are used mainly in neural networks and especially Deep Neural Networks. For this course Linear Algebra will provide the foundation for future topics to understand and work with more complex topics. Having a mathematical background will help you better understand the underlying principles of some of the techniques we will discuss as well as how to work with data.



Some type of ML problems involve solving equations that have the form
$$
a_1*x_1+\dots+a_n*x_n=b
$$
e.g.
$$
5*x_1+2*x_2+100*x_3=100
$$


such an equation can be simplified and we can represent some of the elements as a **vector**
$$
\begin{bmatrix}
5\\
2\\
100
\end{bmatrix} \times x=100
$$


A **vector** is an array of elements 
$$
\begin{bmatrix}
5\\
2\\
100
\end{bmatrix}
$$

```
import numpy as np
np.array([5,2,100])
```

or more generalized case
$$
\begin{bmatrix}
a_1\\
\dots\\
a_n
\end{bmatrix}
$$


**Vector Addition**

To add two vectors `a` and `b` together we just add their respective components
$$
\begin{bmatrix}
a_1 + b_1\\
\dots\\
a_n + b_n
\end{bmatrix}
$$

```
np.array([10,100,1000])+np.array([5,2,100])
```

similarly for subtraction.

```
np.array([10,100,1000])-np.array([5,2,100])
```

**Scalar**

A scalar is a single element vector such as a single value like an integer, or a float number. Example of a scalar `5`

**Scalar Multiplication**

In a similar way we can multiply a vector by a **scalar**.
$$
\begin{bmatrix}
a_1\\
\dots \\
a_n
\end{bmatrix}
\times 5= 
\begin{bmatrix}
a_1 \times 5\\
\dots \\
a_n \times 5
\end{bmatrix}
$$


```
np.array([5,2,100])*5
```

similarly for division with a scalar.

```
np.array([5,2,100])/5
```

### Product between Vectors

There is no definition for vector multiplication. Instead we have two operations that define the product between vectors. **Dot Product** and **Cross Product**.

**Dot Product** denoted by `.` of two vectors `a` and `b` is given by the formula
$$
\begin{bmatrix}
a_1\\
\dots \\
a_n
\end{bmatrix}
. 
\begin{bmatrix}
b_1\\
\dots \\
b_n
\end{bmatrix}= a_1\times b_1+\dots+a_n\times b_n=\sum_{i=1}^{n}{a_i\times b_i}
$$


```
a=np.array([5,2,100])
b=np.array([10,2,10])
np.dot(a,b)
```
**<span style="color:red">Warning</span> : Mismatching Dimensions cause <span style="color:red">Error</span>**

```
a=np.array([5,2,100])
b=np.array([10,2,10,100])
np.dot(a,b)
```
The dimensions of a is 1 by 3 and the dimensions of b are 1 by 4. **The dimensions of a dot product must match between the two vectors**



**Cross Product**, denoted by `X` of two vectors `a` and `b` is given by the formula
$$
\begin{bmatrix}
a_1 \\
a_2\\
a_3
\end{bmatrix}
\times
\begin{bmatrix}
b_1 \\
b_2\\
b_3
\end{bmatrix}=\begin{bmatrix}
a_2 . b_3 - a_3 . b_2 \\
a_3 . b_1 - a_1 . b_3\\
a_1 . b_2 - a_2 . b_1
\end{bmatrix}
$$


```
a=np.array([5,2,100])
b=np.array([10,2,10])
np.cross(a,b)
```
Differences between cross and dot product.

The **Dot product** is defined for arbitarry large vectors and always returns a **scalar**. 

The **cross product** that is only defined for 3 component vectors and returns **another vector**

**<span style="color:red">Warning</span> : Mismatching Dimensions cause <span style="color:red">Error</span>**

```
# ERROR AHEAD - DIMENSIONS MUST BE 2 or 3

a=np.array([5])
b=np.array([10,100])
np.cross(a,b)

# ERROR AHEAD - DIMENSIONS MUST BE 2 or 3

a=np.array([5,2,100])
b=np.array([10,2,10,100])
np.cross(a,b)
```
**<span style="color:red">Warning</span> : Mismatching Dimensions don't always cause <span style="color:red">Error</span>**

```
a=np.array([5,100,1])
b=np.array([10,100])
np.cross(a,b)
```
**EQUIVALENT TO**

```
a=np.array([5,100,1])
b=np.array([10,100,0])
np.cross(a,b)
```

### Geometric Interpretation

**Dot Product** is a number that signifies the length of the vector that is calculated by finding the projection of second vector onto the first vector when multiplied by the first vector's length. 

![dotproduct](../image/dotproduct.gif)

**Cross Product** produces a vector that is perpendicular to both a and b.

![cross_product_hd](../image/cross_product_hd.gif)

Source of both animations is an amazing Youtube channel that has a lot of cool math: [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)

**Matrix** is a 2-d *vector*, or 2-d array. A matrix has columns and rows.
Example of a square matrix with `n x n` dimensions
$$
\begin{bmatrix}
a_{1,1} & \dots & a_{1,n} \\
\dots  & \dots & \dots\\
a_{n,1} & \dots & a_{n,n} \\
\end{bmatrix}
$$
In python we can define

```
np.array([[5,1,1],[3,2,1],[5,5,5]])
```

Matrix addition and subtraction, we add/subtract the elements pair-wise. The matrices must match in dimensions.
**Example**
$$
\begin{bmatrix}
1 & 3 \\
1 & 0 \\
1 & 2\\
\end{bmatrix}
+
\begin{bmatrix}
0 & 0 \\
7 & 5 \\
2 & 1\\
\end{bmatrix}=\begin{bmatrix}
1+0 & 3+0 \\
1+7 & 0+5 \\
1+2 & 2+1\\
\end{bmatrix}=
\begin{bmatrix}
1 & 3 \\
8 & 5 \\
3 & 3\\
\end{bmatrix}
$$

```python
import numpy as np
a=np.array([
    [5,1,1],
    [3,2,1],
    [5,5,5]])
b=np.array([
    [1,1,1],
    [10,10,10],
    [100,100,100]])
# Addition
a+b
# Subtraction
a-b
```

To multiply a matrix by a **scalar** we simply multiply every element of the matrix with that **scalar**

**Example**
$$
\lambda=2, A=\begin{bmatrix}
a & b \\
c & d\\
\end{bmatrix}
\implies A\times \lambda=\begin{bmatrix}2\times a & 2 \times b \\
2 \times c & 2 \times d \\
\end{bmatrix}
$$

```
#MULTIPLICATION
b*2
#DIVISION
b/2
```

**Matrix Multiplication** between two matrices requires both matrices to have matching inner dimensions. If we have a matrix that is `n * m` we can only multiply it with matrices that have dimensions `m * k`. The resulting matrix will have dimensions `n*k`

The element at position `i,j`  on the resulting matrix is calculated by calculated the sum of the product between elements at row `i` from the first matrix and column `j` from the second matrix.

**Formula**
$$
a_{i,j}=\sum_{k=1}^{m}{a_{ik}b_{kj}}
$$
![](../image/15.png)
```
np.matmul(a,b)
```
**<span style="color:red">Warning!</span> The dimensions should match**

```
# ERROR AHEAD - MISMATCHED DIMENSIONS
c=[[1,1],[10,10],[100,100]]
np.matmul(c,a)
```
```
# ERROR AHEAD - MISMATCHED DIMENSIONS
d=[[3,2,1],[5,5,5]]
np.matmul(b,d)
```
**The following works!**

```
np.matmul(d,b)
np.matmul(a,c)
```
**Why?** Inner dimensions match. (1x **3**, **3** x3)



The **transpose** of a matrix is a special operation by which we rotate the matrix along the diagonal.

**Example**

![](https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif)

```
print(a)
print()
print("Transpose")
print(a.transpose())
```
### System of Linear Equations
Linear Algebra helps us solve systems of equations for which there are multiple unknowns. We can convert a system of equations into a matrix and solve it as a matrix problem.

**Example**
$$
x + y + z = 6 \\

2y + 5z = −4 \\

2x + 5y − z = 27\\
$$
We can convert the above system into a matrix with just the coefficients.
$$
\begin{bmatrix}
1 & 1 & 1\\
0 & 2 & 5 \\
2& 5 & -1 \\
\end{bmatrix}
\times
\begin{bmatrix}
x \\
y \\
z \\
\end{bmatrix}
= \begin{bmatrix}
6 \\
-4 \\
27 \\
\end{bmatrix}
$$
we can write this as

$$
A\times X=Y
$$


We can use numpy solve such a system.

```
alpha=np.random.rand(10,10)
y=np.random.rand(10)
np.linalg.solve(alpha, y)
```

**Why Linear Algebra and Machine Learning**

Linear Algebra is used to calculate coefficients of neural networks and on operations called back-propagation. Is especially useful for game-development since a lot of these concepts deal with geometric interpertations, such as physics engines and calculating torque, or other forces acted upon an object. Your GPU is a powerhouse of Linear Algebra that is designed to be able to perform these operations much faster than your CPU. 

In more simple cases of Machine Learning we will examine in class we will use mostly the basic notation of the concepts above. Such as how to solve such a system. 

## Probability

Probability tell us how likely an event is to happen. Probability takes values between 0 and 1. We write probability of an event A (such as tossing a coin and getting heads) as P(A). 

Probability of an event A, then signifies what is the likelyhood of such an event occuring, naturally when the probability is 0, $P(A)=0$ the event is unlikely to happen at all when it is $P(A)=1$ the event is certain to happen.

The probability of flipping a coin on which both sides are heads and getting tails is 0

The probability of flipping a coin on which both sides are heads and getting heads is 1

The probability of flipping a **fair**(well-balanced, ideal,non-biased) coin and getting either heads or tails is $P(A)=0.5$ 

### Sample Space
Sample Space defines all the possible outcomes in an experiment. If we are tossing a coin all the possible outcomes are head and tail. Depending on how we set up our experiment, we can also end up with a coin on the side, or even lose the coin during the flip and if we think hard enough we can come up with more outcomes we can account for.

If we toss a **fair** coin we have P(H)=P(T)=0.5 for the outcomes of **H** ead and **T** ail.

If we add up all probabilities in a sample space they should always equal to 1.

**Example**

$P(S)=P(H)+P(T)=1$

If we toss two coins the possible outcomes increase in size

$S= HH, HT, TH, TT$ 

in the case we want to find the probability of getting two heads in a row, the probability is 

$P(HH)=P(HT)=P(TH)=P(TT)=0.25$

To calculate the probability of an event occurring we use the formula
$$
P(E)=\frac{\text{total number of outcomes in E}}{\text{total number of outcomes in S}}
$$
**S**  is our sample space

**E** is the set of total outcomes of event E from sample space S that we are interested in.

In our example above, we are interested in 2 heads in a row. In that case 

$E=\{H H \}$ and hence $P(E)=\frac{1}{4}$

If we were interested in the probability of getting either two heads or two tails in a row, we would have $E=\{HH,TT\}$ and $P(E)=\frac{1}{2}$

Let's do the same in python

```python
# Generate the sample space
import itertools

x=["H","T"]
sample_space=list(itertools.product(x, repeat=3))

# what is the likelyhood I will NOT get the same flip in a row e.g. HHT THH TTH etc
# we are only interested then in THT, HTH

probability=2/len(sample_space)
print("The probability is: %f"%(probability))

```

**Another Example**

Let's assume I roll a dice and I want to find the probability of getting an even number

```python
import numpy as np
# we add one to the sample space because range starts from 0
sample_space=np.array(range(6))+1 
# the operation % is the mod (modulo), when a number mod 2 is zero, then it is an even number
outcomes=sample_space[sample_space%2==0] 
probability=len(outcomes)/len(sample_space)
print("The probability is: %f"%(probability))
```



## Conditional Probability 

Two events are **Independent** if the probability of one occurring, does not affect the probability of the other occurring. Example is flipping a coin. Each coin flip does not affect the probability of the next coin-flip. We denote conditional probability of two events, A and B as $P(A|B)$ which means the probability of A occuring given that B has occurred. Naturally for indepedent events it doesn't matter if B has occured or not so the probability of A occuring remains the same. 
$$
P(A|B)=P(A)
\\
P(B|A)=P(B)
$$
In the case of a coin, $P(A|B)=P(B|A)=P(A)=P(B)$

**Dependent** events, are events for which the probability of one event occurring affects the probability of the other event occurring. Consider the probability of drawing a card from a given deck of cards. After drawing the first card the probability that any of the other cards will be drawn is changed.
$$
P(A|B)=\frac{P(A \text{ and } B)}{P(B)}
$$

### Example

We receive an email and it is marked as spam. In any given day we receive 392 emails out of which 40 are spam, the spam filter has 90% success rate. 

1. What is the probability that any given email is spam?
2. What is the probability that an email is marked as spam? 
3. What is the probability that an email is marked as spam and is indeed spam? 
4. What is the probability that an email marked as spam is indeed spam?

#### Solution

1. The probability of any given email to be spam is $P(spam)=\frac{\text{total number of spam outcomes}}{\text{total number of emails received}}=\frac{40}{392}=0.1$

2. The number of emails marked as spam are 90% of the actual spam emails given the success rate of our spam filter, that is $\text{number of marked as spam}=0.9*40=36$. The probability that any email is marked as spam is $P(\text{marked as spam})=\frac{\text{number of emails marked as spam}}{\text{total number of emails}}=\frac{36}{392}=0.092$

3. The probability that both occure, is the success rate of our spam filter $P(\text{marked as spam } \textbf{and} \text{ spam})=0.9$

4. The probability we are trying to calculate is $P(spam|\text{marked as spam})$, that is what is the probability that an email is spam, given that it is marked as spam. $P(spam|\text{marked as spam})=\frac{P(\text{marked as spam } \textbf{and} \text{ spam})}{P(spam)}=0.9/0.1=0.09$ 

   Which is about 9%, we can see that our spam filter doesn't perform all that great, a 90% success rate with such low volume of spam emails is not all that great. Another intuitive way to think about it would be a spam filter that classifies everything as spam, would have a success rate of 100%, but would that be a useful spam filter? This concept has to do with evaluating our results and calculating **False Positives**. High false positives lead to bad conditional probability.	



## Derivative

A Derivative of a function $y=f(x)$ describes the rate of change of the function with respect to the rate of change of a variable x. That is, how fast and in what direction does $y$ change as I change the value of x. The derivative then is simply the rate of change of y, and the method of finding the derivative is by diferentiating y. 

In case of a line whose equation is $y=m\times x +b$, the derivative of y is simply the slope $m$, it is the rate of change of y. 

The derivative of y is denoted as $f'(x)$ and in this case we have $f'(x)=m=\frac{\Delta y}{\Delta x}$

For higher polynomial functions, that is functions that are much more complex in how their rate of change, changes over different values, we are interested at the derivative at a specific point of x. 

The derivative in such cases is defined as 
$$
m=\frac{\Delta f(x)}{\Delta x}=\frac{f(x+h)-f(x)}{(x+h)-h}=\frac{f(x+h)-f(x)}{h}
$$
In this case we take the rate of change of x to go from $x\implies x+h$

The value of h, we can choose, but since we are only interested in a very small value of said $h$ we will take a value very close to zero, example: $0.000001$, because we can't have zero value as a denominator we will use a notation called limit to describe the action of taking really small values of h. We write then that the derivative of the function f is.
$$
f'(x)=\lim_{h\rightarrow 0}{\frac{f(x+h)-f(x)}{h}}
$$
Example, calculate the derivative of $f(x)=x^2$ when $x=3$
$$
f'(x)=\lim_{h\rightarrow 0}{\frac{f(3+h)-f(3)}{h}}=\lim_{h\rightarrow 0}{\frac{9+6h+h^2-9}{h}}=\lim_{h\rightarrow 0}{6+h}=6
$$
In this case we know that the rate of change of f'(x) is 6, when x=3. But what does that mean?

The derivative is the value of the slope at a given point. 

![image-20190719093520351 (../image/image-20190719093520351 (1).png)](../../../Downloads/image-20190719093520351 (1).png)

The larger the value of the derivative, the larger the rate of change. 

​	A positive derivative means that the function is increasing.

A negative derivative means that the function is decreasing.

A zero derivative could mean, that the point is **one** of the following

*  **Local minimum**: It is the lowest point in the area sourounding it
* **Local maximum**: It is the highest point in the area sourounding it
* **Global minimum**: It is the lowest possible point of the function
* **Global maximum**: It is the largest possible point of the function

![image-20190719093520351](../image/image-20190719093520351-3496796.png)

Instead of having to calculate the above at every single point for a function, we have some rules that simplify the way we can calculate the derivative. 

### Basic Derivative Rules

| Rule Name  | Function                 | Derivative                                                 |
| ---------- | ------------------------ | ---------------------------------------------------------- |
| Constant   | $f(x)=c$                 | $f'(x)=0$                                                  |
| Power      | $f(x)=x^n$               | $f'(x)=n\times x^{n-1}$                                    |
| Sum        | $f(x)=g(x)+h(x)$         | $f'(x)=g'(x)+h'(x)$                                        |
| Difference | $f(x)=g(x)-h(x)$         | $f'(x)=g'(x)-h'(x)$                                        |
| Product    | $f(x)=g(x)\times h(x)$   | $f'(x)=g'(x)\times h(x) + g(x)\times h'(x)$                |
| Quotient   | $f(x)=\frac{g(x)}{h(x)}$ | $f'(x)=\frac{g'(x)\times h(x) - g(x)\times h'(x)}{h^2(x)}$ |
| Chain      | $f(x)=g(h(x))$           | $f'(x)=g'(h(x))\times h'(x)$                               |

There are more rules, but for now is mostly memorizing them. The proof on how we derive on those results is omitted because is outside the scope of this class.

**The gist of it is that you can take the derivative of any function f(x) and end up with a function f'(x) which describes the rate of change of f(x) at any point x.**

**Example**

Find the **global minima** of the function $f(x)=\frac{x^3-x_{true}^3}{n}$

**Solution**

1. Using the **Constant** rule we can ignore $\frac{1}{n}$ so $f'(x)=\frac{1}{n}\times (x^3-x_{true}^3)'$
2. Using the **Difference** rule we can differentiate $(x-x_{true})$ , $f'(x)=\frac{1}{n}\times ((x^3)'-(x_{true}^3)')$
3. Using the **Constant** rule, we have $(x_{true}^3)'=0$ 
4. Using the **Power** rule we have $(x^2)'=3\times x^2=3\times x^2$
5. Then $f'(x)=\frac{3\times x^2}{n}$

**Plotting $f'(x)$  for n=100**

In order to plot the function we need to pass sample points of x to calculate $f'(x)$ we do this using `numpy.linspace` which generates points within a range at given intervals 

We want to use `plt.plot` in this case which is a line/plot as opposed to a scatter plot. But for this example we will compare both

```python
import matplotlib.pyplot as plt
import numpy as np
x=np.linspace(-10,10,num=1000)
# print(x) # prints a really large array of 1000 numbers, uncomment to see what x looks like
dfx=3*x**2/100
# print(dfx) # prints a really large array of 1000 numbers, uncomment to see what dfx looks like
plt.plot(x,dfx)
```

![image-20190719164439208](../image/image-20190719164439208.png)

if we were to do a scatter plot with 10 points, it would become more obvious how this plot was generated. 

```python
import matplotlib.pyplot as plt
import numpy as np
x=np.linspace(-10,10,num=10)
#print(x)
dfx=3*x**2/100
plt.scatter(x,dfx)
```

![image-20190719164650137](../image/image-20190719164650137.png)

To find the global minimum point of our derivative in this case we can simply use the python `min` function.

```python
print(min(dfx))
```

or we can use the numpy min function

```python
print(dfx.min())
```

Derivatives are important for Machine Learning, because we often use a function to describe something we will look into future chapters that we call **loss**, our goal is to constantly minimize it. While most of the calculations of the derivative and how to minimize the loss is done by existing algorithms, we want to have a good understanding of what that means conceptually to be able to choose the correct parameters to train a Machine Learning model.


